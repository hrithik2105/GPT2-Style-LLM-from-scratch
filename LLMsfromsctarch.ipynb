{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hrithik2105/GPT2-Style-LLM-from-scratch/blob/main/LLMsfromsctarch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1JdqIEX8hSx"
      },
      "source": [
        "*Creating LLMs*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZRzHHRX8n1w"
      },
      "source": [
        "# **Step 1: Tokenization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9c4FkkiGBXT"
      },
      "source": [
        "a) Extract all the characters and store in raw_text (str format):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYQ4iw5h8cxS",
        "outputId": "d5701e84-77c6-4caf-f84c-d8eb40e07a4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Totat number of characters: 20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ],
      "source": [
        "with open (\"/content/the-verdict.txt\", \"r\", encoding = \"utf-8\") as f:\n",
        "  raw_text = f.read()\n",
        "\n",
        "print(\"Totat number of characters:\",len(raw_text))\n",
        "print(raw_text[:99])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwjXUZ99F9i5"
      },
      "source": [
        "b) Use Regex library to split the characters into tokens based on parameters like whitespaces(\\s) and other special characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wewC2jQr9lR3"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "preprocessed  = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PATdajS6m6O9"
      },
      "source": [
        "c) Use **.strip()** to remove white spaces in the list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImvY6VlqnG0d",
        "outputId": "15dfff83-5b94-43be-a5d0-b3465bd7ca74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in'] \n",
            " 4690\n"
          ]
        }
      ],
      "source": [
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(preprocessed[:30], \"\\n\", len(preprocessed))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFwgAqdhO-Xv"
      },
      "source": [
        "# **Step 2: Assigning Token IDs to the tokens**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1047ztaARC7K"
      },
      "source": [
        "a) Sort the 'preprocessed' **list** and store it in a **set** named 'vocab':"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kr3cVUX3IdNF",
        "outputId": "c94c82ff-df9f-4718-cda2-11c842557497"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1130\n"
          ]
        }
      ],
      "source": [
        "vocab = sorted(set(preprocessed))\n",
        "print(len(vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ITggCNVTw0N"
      },
      "source": [
        "b) Use **enumarate()** function to assign a number (starting from 0) to all the tokens in vocab **set** and store it in vocab **dictionary**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHb1Gt6dQzGo"
      },
      "outputs": [],
      "source": [
        "vocab = {token:integer for integer, token in enumerate(vocab)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QMWdt4LVVEe"
      },
      "source": [
        "c) Print first 25 tokens (with their ids) from the dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zunqs3lpVHzG",
        "outputId": "8acad061-0fc4-4396-ad58-6848c04e392e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('!', 0)\n",
            "('\"', 1)\n",
            "(\"'\", 2)\n",
            "('(', 3)\n",
            "(')', 4)\n",
            "(',', 5)\n",
            "('--', 6)\n",
            "('.', 7)\n",
            "(':', 8)\n",
            "(';', 9)\n",
            "('?', 10)\n",
            "('A', 11)\n",
            "('Ah', 12)\n",
            "('Among', 13)\n",
            "('And', 14)\n",
            "('Are', 15)\n",
            "('Arrt', 16)\n",
            "('As', 17)\n",
            "('At', 18)\n",
            "('Be', 19)\n",
            "('Begin', 20)\n",
            "('Burlington', 21)\n",
            "('But', 22)\n",
            "('By', 23)\n",
            "('Carlo', 24)\n",
            "('Chicago', 25)\n"
          ]
        }
      ],
      "source": [
        "for i, item in enumerate(vocab.items()):\n",
        "  print(item)\n",
        "  if (i>=25):\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GETe6KxlriV"
      },
      "source": [
        "d) Extending the vocab by adding <|unk|> (token used to denote any unknown token from the input) and <|endoftext|> (token used when one source/segment/topic is over). We do this by converting the preprocessed set back to list and then using the .extend() function to add the new items."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Os-z1KpVbxK"
      },
      "outputs": [],
      "source": [
        "preprocessed = sorted(list(set(preprocessed)))\n",
        "preprocessed.extend([\"<|unk|>\",\"<|endoftext|>\"])\n",
        "vocab = {token:integer for integer, token in enumerate(preprocessed)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DanVUo5OpwvF",
        "outputId": "c043b15c-db46-4f8f-bb45-485e6847b895"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1132"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(vocab.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtJm9-S3qHBw",
        "outputId": "bc50a63a-6859-4683-d086-d48c819cd509"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('younger', 1127)\n",
            "('your', 1128)\n",
            "('yourself', 1129)\n",
            "('<|unk|>', 1130)\n",
            "('<|endoftext|>', 1131)\n"
          ]
        }
      ],
      "source": [
        "for i, item in enumerate(list(vocab.items())[-5:]):\n",
        "  print(item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BdbyT8NJUki"
      },
      "source": [
        "#Importing **tiktoken**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEz27gzrb6P5"
      },
      "source": [
        "Tiktoken library is currently used by OpenAI which relies on **byte-pair encoding** which supports all the GPT models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMEYa6K4q6Bu",
        "outputId": "ad470136-15f5-44dd-ab0d-584f82d6c97e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip3 install tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D36v2OGum9oW"
      },
      "source": [
        "# Implementing **BPE** and decoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1f9O1upcelD"
      },
      "source": [
        "Byte-pair encoding is a sub word tokenization technique used by the LLM. Subword basically captures rootword, suffixes, repeated words and assigns a token value to them. Subwords are created based on the token frequency of characters. This makes the vocab shorter, computationally efficient and retain meanings of rootwords and suffixes. Eg: 'Boy','Boys' are fed into the LLM. Here, 'Boy' is assigned one token id and 's' is assigned another token id. The token ids can look like [3476, 3476, 56] for 'Boy','Boy' and 's'. This is called byte-pair encoding. Initially 'B' is merged with 'o' which gives 'Bo' (1 char should be merged with another. Hence byte-pair). Later, 'Bo' is merged with 'y' resulting in 'Boy' token. All the previous tokens relating to B, o and y are deleted. Only 'Boy' is kept as a single token along with the previous 's' token.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEwHtlfocXZK",
        "outputId": "15a99e45-d429-47ad-c0a4-b04ac371a1ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tiktoken version: 0.9.0\n"
          ]
        }
      ],
      "source": [
        "import importlib\n",
        "import tiktoken\n",
        "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YI2vKYObhHmS"
      },
      "source": [
        "We will extract the GPT-2 version from the tiktoken library into the tokerizer variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1LCNG0D3ha7C"
      },
      "outputs": [],
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMRbaNzsgXc7"
      },
      "source": [
        "The tiktoken library has **encode** (encodes token to token ids) and **decode** (decodes token ids to tokens) methods in it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FZcs_i8km7p"
      },
      "source": [
        "Using the tokenizer variable to access the encode method from the tiktoken library over a sample text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raPpVcJxgUpP",
        "outputId": "f1b6bf96-94c1-4185-9984-7aa737885f61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[34, 1585, 10115, 36309, 318, 262, 6000, 1683, 4346, 2137, 287, 262, 995, 13, 50256, 679, 468, 7781, 625, 47679, 4661, 13]\n"
          ]
        }
      ],
      "source": [
        "sample_text = (\"Cristiano Ronaldo is the greatest ever football player in the world.\"\n",
        "                \"<|endoftext|> He has scored over 920 goals.\")\n",
        "\n",
        "tokenids = tokenizer.encode(sample_text, allowed_special={\"<|endoftext|>\"})\n",
        "print(tokenids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_F1woPGKk1W1"
      },
      "source": [
        "Note: We can also see the token id for <|endoftext|> token which is 50257 which is basically present in the end. So the vocab of GPT-2 consists of 50257 tokens. The total number of words in English language is around 170,000 - 200,000 words. So, this byte-pair encoding method efficiently and effectively reduced the vocab size retaining important rootwords and suffixes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDnpCcfJmOSC"
      },
      "source": [
        "Inversely, implementing decode method to extract the the tokens back from the tokenids."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCZAI-U8jbYi",
        "outputId": "74fab50b-68e2-422a-9836-f7ca874f4f6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cristiano Ronaldo is the greatest ever football player in the world.<|endoftext|> He has scored over 920 goals.\n"
          ]
        }
      ],
      "source": [
        "tokens = tokenizer.decode(tokenids)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YC4uwDLjnwx"
      },
      "source": [
        "# Sample implementation of **Input-Target** pairs using **Sliding** **Window** approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBlICPXmng_u",
        "outputId": "042ac8db-d289-422e-c164-c38695d75a9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5145\n",
            "[40, 367, 2885, 1464, 1807, 3619, 402, 271, 10899, 2138, 257, 7026, 15632, 438, 2016, 257, 922, 5891, 1576, 438]\n"
          ]
        }
      ],
      "source": [
        "enc_text = tokenizer.encode(raw_text)\n",
        "print(len(enc_text))\n",
        "print(enc_text[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa4zj-EG9a9t"
      },
      "source": [
        "Sample creation of Input-Target pairs using the same verdict dataset but removing first 50 tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkBUnQcYj5FG"
      },
      "outputs": [],
      "source": [
        "sample_enc = enc_text[50:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adBJfq9E-QIn"
      },
      "source": [
        "We are going to use 2 vairables x (denoting the input tokens) and y (denoting the target tokens). Eg: Lets say X=[1,2,3,4] and Y=[2,3,4,5]. We want the first element in i/p token to match first value in the o/p token in 1st iteration. In the next iteration, we want to consider the 1st and the 2nd token from i/p and only 2nd token in the o/p (rest of the o/p values should be masked). Context size lets us choose how many tokens should be considered in each iteration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2fUYhlM-PmD",
        "outputId": "b4aa27e4-c4c6-47ac-f87a-5bffde111cd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input tokens:[290, 4920, 2241, 287]\n",
            "target tokens:[4920, 2241, 287, 257]\n"
          ]
        }
      ],
      "source": [
        "context_size = 4\n",
        "x = sample_enc[:context_size] #sample_enc[0 to 4] basically 0,1,2,3\n",
        "y = sample_enc[1:context_size + 1] #sample_enc[1 to 5] basically 1,2,3,4\n",
        "print (f\"input tokens:{x}\")\n",
        "print (f\"target tokens:{y}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkZvPpOrAfT8"
      },
      "source": [
        "This approach is called 'Sliding Window' approach which basically slides over the context_size in each iteration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kZuSJeNECx8"
      },
      "source": [
        "We can show it in step-by-step process to show the iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAqJRGNYAZgX",
        "outputId": "ba1900b3-d3f6-4d13-84e2-a689a553d2fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[290] ------> 4920\n",
            "[290, 4920] ------> 2241\n",
            "[290, 4920, 2241] ------> 287\n",
            "[290, 4920, 2241, 287] ------> 257\n"
          ]
        }
      ],
      "source": [
        "for i in range (1, context_size+1):\n",
        "  context_input = sample_enc[:i] # Displays all the elements from sample_enc list till i (excluding i).\n",
        "  desired = sample_enc[i] # Displays i whch is basically the output token.\n",
        "\n",
        "  print(context_input, \"------>\", desired)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXf8CiKhGAig"
      },
      "source": [
        "Inversely, decoding in step-by-step process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3pg7iE8FyCQ",
        "outputId": "324977b1-2cb3-4ae9-97bb-7b3657e340ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " and ------>  established\n",
            " and established ------>  himself\n",
            " and established himself ------>  in\n",
            " and established himself in ------>  a\n"
          ]
        }
      ],
      "source": [
        "for i in range(1, context_size+1):\n",
        "  context_input = sample_enc[:i]\n",
        "  desired = sample_enc[i]\n",
        "\n",
        "  print(tokenizer.decode(context_input), \"------>\", tokenizer.decode([desired]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qp9aqkksu3YD"
      },
      "source": [
        "# Multi-dimensional approach using **Dataset** and **Dataloader**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLrZ1hLavMKL"
      },
      "source": [
        "We can implement the same Input-Target pairs using Dataloader which uses Dataset from pytorch library(which basically works on the sliding window approach). We do this because we want to convert our tokenized ids (input) into multidimensional arrays or **Tensors**. This step is crucial before vector embeddings which takes in these **Tensors** as input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IuSpX_UXvGIg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "      self.input_ids = []\n",
        "      self.target_ids = []\n",
        "\n",
        "      token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftxt|>\"}) #Tokenizing the entire input text\n",
        "\n",
        "      # Using sliding window to chunk the overlapping sequences of max_length(=context size)\n",
        "      # We iterate it over len(token_ids)-max_length to not exceed the available tokens which can lead to out of bounds error\n",
        "      for i in range(0, len(token_ids) - max_length, stride):\n",
        "        input_chunk = token_ids[i: i + max_length]    #From first tokenid till context_size\n",
        "        target_chunk = token_ids[i + 1: i + max_length + 1] #From 2nd element till context_size+1\n",
        "\n",
        "        self.input_ids.append(torch.tensor(input_chunk))\n",
        "        self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):  #Returns the length of the input tensor\n",
        "      return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):   #Returns the i/p tensor(multi-dim array) and its corresponding o/p tensor(multi-dim array)\n",
        "      return self.input_ids[idx], self.target_ids[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxu1OTeD2E-y"
      },
      "source": [
        "Feeding the input Dataset to the Dataloader in batches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSpjPALh7lEH"
      },
      "source": [
        "**batch_size** = How many sequences of tensors to consider at once which can run in parallel. Default = 4.\n",
        "\n",
        "**max_length** = How many token ids in each row of tensor\n",
        "\n",
        "**stride** = how many steps to take from one input tensor to process the next input tensor (step size). Taking too little stride results in more overlapping of tensors with very little change in each.\n",
        "\n",
        "**drop_true** = **True** ensures all batches have equal number of sequences(rows of tensors). Incase if its uneven, it gets dropped to prevent any inconsistencies in training as the heavily depends on equal batch sizes. It's also helpful in padding to prevent any erros.\n",
        "\n",
        "num_workers = number of parallel\n",
        "\n",
        "\n",
        "Eg: Lets assume we have token_ids = [101, 2009, 2001, 1037, 3835, 2154, 102]\n",
        "\n",
        "with max_length = 3 and stride = 1\n",
        "\n",
        "In sequence 1, input tokens = [101, 2009, 2001] and the resulting target tokens = [2009, 2001, 1037]\n",
        "\n",
        "In sequence 2, input tokens = [2009, 2001, 1037], target tokens = [2001, 1037, 3835]\n",
        "\n",
        "In sequence 3 input tokens = [2001, 1037, 3835],\n",
        "target tokens = [1037, 3835, 2154]\n",
        "\n",
        "In sequence 4 input tokens = [1037, 3835, 2154],\n",
        "target tokens = [3835, 2154, 102].\n",
        "\n",
        "Here, giving batch_size = 4 means that the dataloder will process all 4 sequences(rows of tensor) in one iteration to produce the target tokens for all 4 sequences where one tensor is overlapping another."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BnDtXlBi57Ci"
      },
      "outputs": [],
      "source": [
        "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128,\n",
        "                         shuffle=True, drop_last=True, num_workers=0):\n",
        "\n",
        "  # Initialize tokerinzer\n",
        "  tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "  #Create dataset\n",
        "  dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "  dataloader = DataLoader(dataset, batch_size=batch_size,\n",
        "                          shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
        "\n",
        "  return dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWGK22usPi_O"
      },
      "source": [
        "Using the dataloader function to create and print a sample tensor using input from verdict.txt. The batches are going to be displayed depending on the batch size. 2 batch size = 2 rows with each row = max_length. The second batch depends on the stride."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNxpaUNa-dU_",
        "outputId": "b0dfb978-55f6-48e4-b586-90e9646025ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
          ]
        }
      ],
      "source": [
        "dataloader = create_dataloader_v1(raw_text, batch_size = 1, max_length=4, stride=2, shuffle=False)\n",
        "\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUL2E9sESLVW"
      },
      "source": [
        "Printing second batch tensor. We use **iter** (to iterate over the batches one by one using the **next**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqnv7tWARApR",
        "outputId": "d06dfb75-d308-4330-b826-8269f92fd23f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[tensor([[2885, 1464, 1807, 3619]]), tensor([[1464, 1807, 3619,  402]])]\n"
          ]
        }
      ],
      "source": [
        "second_batch = next(data_iter)\n",
        "print(second_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LF9m_JaBcmgs"
      },
      "source": [
        "We can vary strides, batch sizes and strides to produce tensors of varying length and step sizes. Normally, its better to keep max_length and stride as same number because that way the sequences (tensors)dont over lap after each iteration and complete tensors will be displayed without skipping any tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CttqASF2SaMq",
        "outputId": "42a44dab-adbe-4931-ba14-aca2c9bc6850"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First Batch: [tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]]), tensor([[  367,  2885,  1464,  1807],\n",
            "        [ 3619,   402,   271, 10899],\n",
            "        [ 2138,   257,  7026, 15632],\n",
            "        [  438,  2016,   257,   922],\n",
            "        [ 5891,  1576,   438,   568],\n",
            "        [  340,   373,   645,  1049],\n",
            "        [ 5975,   284,   502,   284],\n",
            "        [ 3285,   326,    11,   287]])]\n",
            "Second Batch: [tensor([[  287,   262,  6001,   286],\n",
            "        [  465, 13476,    11,   339],\n",
            "        [  550,  5710,   465, 12036],\n",
            "        [   11,  6405,   257,  5527],\n",
            "        [27075,    11,   290,  4920],\n",
            "        [ 2241,   287,   257,  4489],\n",
            "        [   64,   319,   262, 34686],\n",
            "        [41976,    13,   357, 10915]]), tensor([[  262,  6001,   286,   465],\n",
            "        [13476,    11,   339,   550],\n",
            "        [ 5710,   465, 12036,    11],\n",
            "        [ 6405,   257,  5527, 27075],\n",
            "        [   11,   290,  4920,  2241],\n",
            "        [  287,   257,  4489,    64],\n",
            "        [  319,   262, 34686, 41976],\n",
            "        [   13,   357, 10915,   314]])]\n"
          ]
        }
      ],
      "source": [
        "dataloader2 = create_dataloader_v1(raw_text, batch_size = 8, max_length=4, stride=4, shuffle=False)\n",
        "\n",
        "data_iter = iter(dataloader2)\n",
        "\n",
        "first_batch = next(data_iter)\n",
        "second_batch = next(data_iter)\n",
        "\n",
        "print(\"First Batch:\",first_batch)\n",
        "print(\"Second Batch:\",second_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9-lx5HbfM1Z"
      },
      "source": [
        "#Creating Vector/Token Embedding Layer for the LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwIUiqNff6yZ"
      },
      "source": [
        "Creating lookup dictionary for the input token tensor. GPT-2 uses vocab size of 50257(each token id having dimension of 256), the overall dimension for our vector/token embeddings will be 50257x256"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSs97xELhPVs"
      },
      "source": [
        "We use **torch.nn.Embedding()** to create vector/token embedding layer(matrix) which takes in vocab_size and output_dim as paramenters which is nothing but 50257x256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wV3Bt-yldYgI"
      },
      "outputs": [],
      "source": [
        "vocab_size = 50257\n",
        "output_dim = 256\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dttDv9MMinzn"
      },
      "source": [
        "Creating the main dataloader of verdict for the LLM training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtxvVvCHiTbI"
      },
      "outputs": [],
      "source": [
        "max_length = 4\n",
        "dataloader_verdict = create_dataloader_v1(raw_text, batch_size = 8, max_length=max_length,\n",
        "                                          stride=max_length, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oc34inz6jzxn"
      },
      "source": [
        "Since the batch_size = 8, each batch will have tensor dimension of 8x4 (8=batch size=number of rows, 4=context_length=max_length=number of columns)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Tg9sB-skoBx",
        "outputId": "a4a0c81e-89e6-45f7-dd70-fd41c800e248"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token Ids: \n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "Shape: torch.Size([8, 4])\n"
          ]
        }
      ],
      "source": [
        "data_iter = iter(dataloader_verdict)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Token Ids: \\n\", inputs)\n",
        "print(\"Shape:\",inputs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1cbwG3jmfjR"
      },
      "source": [
        "Using the embedding layer which was initially created to embed these 8x4 sized tensors. Here each of the token_ids will be matched to a vector embeding in a 256 dimensional space. Essentially, each tensor will have dimensions of 8x4x256 (3D tensor)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjZZVW4ClYG8",
        "outputId": "354b429f-dd46-403b-8b26-d7b17ea36ca8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ],
      "source": [
        "vector_embeddings = token_embedding_layer(inputs)\n",
        "print(vector_embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8G3vx3Rn0bJ"
      },
      "source": [
        "# Adding Positional Embeddings to the Vector Embeddings to create the input embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvsqCxeaoCHm"
      },
      "source": [
        "Positional embeddings are important as tokens appearing at the starting of the input can have different semantic meaning or context compared to tokens appearing at the end of the input. We basically create pos_embeddings based on the size of each row of input (context_size=max_length) being processed. Here, we are mapping 4 token_ids in each row where each token id furter has 256 dimensions which is mapped to the vocab, so we create positional embedding layer of 4x256 dimension.\n",
        "Essentially, we embed the same pos_embedding vector to all the vector embeddings depending on their positions in the matrix. So, 1st column vector embedding will always get added with 1st column pos_embedding. Similarly 2nd column vector_embedding will always get added with 2nd column pos_embedding and so on till 4th column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nalh7VXPnp4c",
        "outputId": "dd69aa58-4f21-4680-df49-998335edc341"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 256])\n"
          ]
        }
      ],
      "source": [
        "context_length = max_length\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
        "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
        "print(pos_embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcqXdw2NtLfm"
      },
      "source": [
        "torch.arrange is used to create 1-D tensors from 0 till context_length-1. In our case context_length = 4[0,1,2,3] and each of them will have 256 dimensions. This is done using torch.arrange."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYFAdgRIu6uK"
      },
      "source": [
        "Creating the final **input_embeddings** by adding the **vector_embeddings** with **pos_embddings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1_onQpsvPL7",
        "outputId": "66b43d92-2f51-4eda-e7ce-de7f9a5eb8b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ],
      "source": [
        "input_embeddings = vector_embeddings + pos_embeddings\n",
        "print(input_embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45NNta7yBSvG"
      },
      "source": [
        "#Causal Attention (single head) for a sample input tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3BWnOUoBlF8"
      },
      "source": [
        "We implement causal attention for a single head for a sample tensor input. This can be extended to a multi-head approach where computations are done in parallel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OYSc6WMCEpV"
      },
      "source": [
        "Note: Causal attention is different compared to self attention as causal attention implements **not** conserving future tokens by intorducing masking to cover the upper triangular attention score matrices  while normal self atention needs access to the future tokens to actually cause an impact (atend to) on the previous tokens. Autoregressive models like GPT uses Causal attention while Bidirectional models like BERT uses self-attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A885Qt-fvjIW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "sample_inputs = torch.tensor(\n",
        "      [[0.43, 0.15, 0.89],    #Your\n",
        "       [0.55, 0.87, 0.66],    #journey\n",
        "       [0.57, 0.85, 0.64],    #starts\n",
        "       [0.22, 0.58, 0.33],    #with\n",
        "       [0.77, 0.25, 0.10],    #one\n",
        "       [0.05, 0.80, 0.55]]    #step\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6v99nzphGQ_S"
      },
      "source": [
        "After creating sample tensor input of dimension 6x3 ( each row corresponds to the input vector embedding of tokens \"Your\", \"journey\", \"starts\", \"with\", \"one\",\"step\". Each column corresponds to the the dimensional space at which they are embedded in. We can see, here to make things easier, they are represented in 3dimensional vector space.) Note: GPTs and other models are generally embedding in higher space(12000+ for GPT3) and also have more context length (more rows)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTmNQwpuHbcz"
      },
      "source": [
        "Before starting the attention mechanism, we kind of assume that the input vector embedding have somehow encoded the meaning of the words given to them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owKeGVqVH5Cv"
      },
      "source": [
        "The main goal of the attention is to give **RICHER** context to the input embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ms6FuIDlJm0c"
      },
      "source": [
        "Attention Pattern (Attention scores) are generated by considering the 3 weighted matrices (Query, Key, Value). As usual these matrices are initialized with random weights as the model learns to adjust them accordingly during training and back propagation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8S2-51aRyhR"
      },
      "source": [
        "**Goal of Query matrix**: Think of query vector like asking question to all the vecotrs in the vector subspace.\n",
        "\n",
        "**Goal of Key matrix**: Think of key matrices like answering to the query by revealing its location in the vector space.\n",
        "\n",
        "**Goal of Value matrix**: To navigate/move the query-key vectors to create a context_vector which has the required richer relationship considering all the input tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CqETH1wW0iv"
      },
      "source": [
        "To make the model easily scalable, we'll create a batch (by stacking the input on top of the same input of same values). This batch is then passed as a parameter to the causal attention function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XJ4ltEcH4SZ",
        "outputId": "022c7ed7-713c-4aab-d293-98894b656b19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 6, 3])\n"
          ]
        }
      ],
      "source": [
        "batch = torch.stack((sample_inputs, sample_inputs), dim=0)\n",
        "print(batch.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RinOE95FXMth"
      },
      "source": [
        "2 represents the number of stacks, 6 represents the context_length dimension (number of tokens considered at once to calculate the attention pattern/score). 3 represents each token dimension in the vector space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iafGuUOSWsrV"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class CausalAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias = False):\n",
        "        super().__init__()\n",
        "        self.d_out = d_out\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal = 1))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        attn_scores = queries @ keys.transpose(1, 2)\n",
        "        attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYP1I8IkcYXU"
      },
      "source": [
        "**Step 1**: Create weighted matrices for query, key and value with the dimenions **d_in X d_out** (d_in should be equal to the dimension of each token = 3 in the case we're gonna define below, d_out can be any value and it represents the overall output dimension of context_vec. we have taken d_out = 2 in sample case defined below). These matrices can be created using torch.nn.Linear() which follows the formula y = x.A**T + b and also taking in the parameters equal to the d_in and d_out. Also, by default bias gets added the weighted matrices, to remove the bias, set bias = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH4WgqGog8il"
      },
      "source": [
        "**Step 2:** Inside **forward** function, the W_key, W_query and W_value matrices all get multiplied with the sample_input tensor, producing, 'keys', 'queries' and 'values' matrices respectively. Note: They have the shape(dimensions) of **b** (batch_size) x **num_tokens** (context_length) x **d_in** (each input token dimension). In our sample_input, we started out with 2x6x3 dimensions where each of the tensor has dimensions of 6x3. Now this 6x3 matrix gets multiplied with 3x2 (initially weighed key, query and value matrices) seperately to produce 6x2 dimensional keys, queries and values matrices sepertely. When stacked, they have the dimension 2x6x2 which denotes batch_size, num_tokens and d_in. Now we dont need the input tensors anymore as their information lies within these 3 matrices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJCS88h8q2gD"
      },
      "source": [
        "**Step 3**: Attention score/Attention pattern is generated by taking the dot-product (@) between queries and keys transpose. Note: Since queries, keys and values matrices have the input dimensions: batch_size x num_tokens x d_in (2 x 6 x 2), considering only the 2nd and the 3rd dimensions which is (1,2) which is basically num_tokens x d_in from the list gives queries(6 x 2) and keys transpose(2 x 6). So the resultant attn_score is now having the dimensions(6 x 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGsu4nZcu59U"
      },
      "source": [
        "**Step 4:** We use *register_buffer()* to create a matrix (mask) full of ones in the upper traingle and zeros in the lower triangle. Diagonal = 1 implies that we shift our diagonal by 1 unit up. This matrix will later used to mask the attn_score matrix to produce attn_weight matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8Tj9UeH7L7L"
      },
      "source": [
        "**Step 5:** We replace 1s (bool) in the mask matrix with -inf and then we will mask it with the attn_scores matrix to create attn_weights matrix. For this *masked_fill_()* fucntion from pytorch is used.\n",
        "\n",
        "Note: [:num_token, :num_token] is given as a range to iterate over the entire mask matrix. We can edit this range for customized masking where we can select the rows and columns to mask."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sB0XxoKR__wl"
      },
      "source": [
        "**Step 6:** We apply **softmax** function to **normalize** the **attn_weight** matrix row by row so that all weights now lie in the range 0 to 1 (includes both 0 and 1). Also, the sum of all weights in each row should be 1. -inf turns to 0 as there is an exponent in the softmax function. (Softmax = (e**x1 - max_weight of that row) divided by the sum of the row currently). Also, the attn_score gets divided by the sqrt(keys matrix 2nd dimension)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPV0mXnXCRvR"
      },
      "source": [
        "**Step 7:** Implementation of dropouts using the **torch.nn.dropout()** function to make sure all neurons are active and none of them become inactive. This is done by dropping out specified percentages of the attn_weight. After dropping out, the function automatically scaled the remaining weights by the amount dropped in order to maintain the total sum of activations of neurons which is constant. Note: weights will get randomly dropped but the probability(percentage) is maintained across the entire attn_weight matrix and not across each row."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZLYMVcEENJl"
      },
      "source": [
        "Step 8: We finally multiply the attn_weight matrix (6x6) with the values matrix (6x2) which produces the final context_vec matrix having dimensions (6x2). 2 batches of these will be generated. Each of 1st dimension is corresponding to the input tokens.\n",
        "This means each row of the contex_vec matrix will correspond to a vector with 2 dimensions. So we get 6 context vectors for 6 tokens with each of them having enriched meanings and relationships. Also the later tokens aren't affecting the earlier tokens in this mechanism."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2cdi_S9GPKd"
      },
      "source": [
        "Using Causal Attention by creating an instance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xv-5dT-jcWWi",
        "outputId": "41dc9fec-eded-45b7-cc59-3d018708f05d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[-0.4519,  0.2216],\n",
            "         [-0.5874,  0.0058],\n",
            "         [-0.6300, -0.0632],\n",
            "         [-0.5675, -0.0843],\n",
            "         [-0.5526, -0.0981],\n",
            "         [-0.5299, -0.1081]],\n",
            "\n",
            "        [[-0.4519,  0.2216],\n",
            "         [-0.5874,  0.0058],\n",
            "         [-0.6300, -0.0632],\n",
            "         [-0.5675, -0.0843],\n",
            "         [-0.5526, -0.0981],\n",
            "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n",
            "\n",
            " torch.Size([2, 6, 2])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "d_in = sample_inputs.shape[1]\n",
        "d_out = 2\n",
        "context_length = batch.shape[1]\n",
        "causal_attn = CausalAttention(d_in, d_out , context_length, 0.0)\n",
        "context_vec = causal_attn(batch)\n",
        "print(context_vec)\n",
        "print(\"\\n\",context_vec.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E27ijflmjMyJ"
      },
      "source": [
        "# Multi-Head attention with weight splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AgVTg9zIIUD4"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias = False):\n",
        "    super().__init__()\n",
        "    assert (d_out % num_heads == 0), \\\n",
        "    \" d_out must be divisible by num_heads\"\n",
        "\n",
        "    self.d_out = d_out\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = d_out // num_heads #Reducing the proj dimension to match the output dim\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
        "    self.W_keys = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
        "    self.W_values = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
        "    self.dropout = nn.Dropout()\n",
        "    # Mask layer consisting of an upper triangluar matrix (lower part = 0, upper part= 1, diagonal = 1 by default).\n",
        "    # putting digonal = 1 in the parameter, shifts the diagonal up by 1 unit. so diagonals will be filled with 0.\n",
        "    self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal = 1))\n",
        "    self.out_proj = nn.Linear(d_out, d_out) #Linear layer to combine output heads\n",
        "\n",
        "  def forward(self, x):\n",
        "    b, num_tokens, d_in = x.shape\n",
        "    queries = self.W_query(x)\n",
        "    keys = self.W_keys(x)\n",
        "    values = self.W_values(x)\n",
        "\n",
        "# Changing the dimensions of keys, queries and values matrices by unrolling/replacing the d_out with num_head,head_dim\n",
        "# Basically we are converting the 3dim tensor to a 4dim tensor\n",
        "\n",
        "    queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "    keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "    values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "# Now we will group by num_heads instead of num_tokens. For this we need to\n",
        "# switch (b, num_tokens, num_heads, head_dim) with (b, num_heads, num_tokens, head_dim).\n",
        "# This is done by taking the transpose between 1 and 2 positions.\n",
        "    queries = queries.transpose(1, 2)\n",
        "    keys = keys.transpose(1, 2)\n",
        "    values = values.transpose(1, 2)\n",
        "\n",
        "#Computing attn_scores by taking dot product queries and keys.transpose. We will consider switching num_tokens and head_dim\n",
        "# queries (b, num_heads, num_tokens, head_dim) will be multiplied with keys transpose (b, num_heads, head_dim, num_tokens).\n",
        "# So the resulting attn_scores dim will be (b, num_heads, num_tokens, num_tokens).\n",
        "# Therefore, for each head, the rows and columns will be num_tokens and num_tokens effectively generating the attention pattern.\n",
        "    attn_scores = queries @ keys.transpose(2, 3)\n",
        "\n",
        "# Implementing a mask layer to prevent the future tokens from affecting the previous tokens.\n",
        "# mask.bool() returns all 1's, so we will replace them with -inf and add that to the attn_scores matrix.\n",
        "    mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "    attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "# computing attn_weights by dividing attn_scores by the head_dim which is at -1 pos in (b, num_heads, num_tokens, head_dim).\n",
        "# dim = -1 is given to make sure each rows (and not cols) sum up to 1.\n",
        "# Normalizing using softmax() which replaces -inf with 0 and also makes sure\n",
        "# all rows add up to 1 with each value between 0 and 1. So even -ve values will be replaced with a +ve one.\n",
        "# The resulting normalized and masked attn_weights dim remains unchanged (b, num_heads, num_tokens, num_tokens)\n",
        "    attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim = -1)\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "# Computing updated attn_weights matrix by\n",
        "# multiplying attn_weights(b, num_heads, num_tokens, num_tokens) with values (b, num_tokens, num_heads, head_dim) matrix.\n",
        "# Resultant matrix will have dim (b, num_heads, num_tokens, head_dim)\n",
        "# We need to swap back num_tokens and num_heads because the final resultant context_vec matrix will have\n",
        "# dim as (b, num_tokens, d_out) where d_out = (num_heads, head_dim)\n",
        "    context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "# contiguous() merges num_heads and head_dim\n",
        "    context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "# combining the output heads\n",
        "    context_vec = self.out_proj(context_vec)\n",
        "\n",
        "    return context_vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUwgq8OM9uFn"
      },
      "source": [
        "Implementing MHA using a sample tensor as input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srE5tBUkwYZp",
        "outputId": "7012a71b-35bd-4635-c03a-fe69794cc475"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 3, 6])\n",
            "tensor([[[-0.0152, -0.1093, -0.2054,  0.0487, -0.4244, -0.3690],\n",
            "         [ 0.1787, -0.0857,  0.1942, -0.1442, -0.2542, -0.4416],\n",
            "         [ 0.1650, -0.0203,  0.2202, -0.0756, -0.3357, -0.3732]],\n",
            "\n",
            "        [[ 0.1716, -0.0724,  0.0353, -0.0056, -0.2458, -0.2790],\n",
            "         [ 0.0618, -0.1808, -0.2494,  0.0312, -0.3029, -0.3069],\n",
            "         [ 0.1300, -0.1126, -0.0586,  0.0230, -0.3148, -0.2657]]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "torch.Size([2, 3, 6])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "inputs = torch.tensor(\n",
        "        [[0.43, 0.15, 0.89, 0.55, 0.23, 0.32],    # Row 1 token\n",
        "         [0.64, 0.98, 0.54, 0.23, 0.54, -0.56],   # Row 2 token\n",
        "         [0.43, 0.15, 0.89, 0.55, 0.87, 0.66]]    # Row 3 token\n",
        ")\n",
        "\n",
        "batch = torch.stack((inputs, inputs), dim=0)\n",
        "print(batch.shape)\n",
        "\n",
        "batch_size, context_length, d_in = batch.shape\n",
        "d_out = 6\n",
        "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
        "context_vec = mha(batch)\n",
        "print(context_vec)\n",
        "print(context_vec.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXLNVmvm_195"
      },
      "source": [
        "This basically gives us multiple contexts (or many different meanings) to the tokens thus making them closer to many other tokens in the vector space. Example: Give just \"Tower\", the model will assume that the tower is a 'large', 'tall', 'steel' structure and tries to give the vector embeddings which are close to 'tall', 'large' and 'steel'. If we just change the input to \"Eiffel Tower\", now the vector will slightly point more towards Paris, France along with the previous embeddings it had with large, tall and steel. If I again change the input to \"Miniature Eiffel Tower\", then the model no longer points towards large, tall and steel. So this change in vector embeddings are done by the Value Matrices mulitplication which basically steers the vectors by providing direction using which it can scale up or down the dimension subspace and then point in the updated direction. There are 2 kinds of value matrices, value up and value down. Value up matrices puts the vector to a higher dimensional space and are generally stiched together to form a single matrix in transformers usually. Value down matrices are usually referred to as output matrix which basically brings down the vector from higher dimensional to lower dimensional space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjYXqiTxoo-x"
      },
      "source": [
        "# Layer Normalization, GELU and FeedForward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icCc31GQozNS"
      },
      "source": [
        "Layer Normalization: We do layer normalization on 3 seperate occasions in the GPT architecture (2 times  inside the Transformer block). First time usage is after receiving the input vector embeddings (input encoding from the lookup consisting of 50257 tokens + positional encodings) but before the Masked Multi-Head Attention. Second time is before the Feed Forward layer. Third time is out of the transformer block which we will implement later. Goal of normalization is to make the mean across the vector embeddings of every token to be 0 and variance to be 1. By doing this we can keep the gradient stable and the learnig process is at a good rate. It also prevents the internal covariate shift which basically changes the input distributions for all layers in the feed forward neural network. The normalization is done by taking the input vectors (dim = 768) for each token and for each vector dimension(weight) subtracting the mean across the columns of that particular row (for each input token basically) with the weight and dividing the difference by the square root of variance of that particular token (across the col). (Variance is calculated by taking the weighted sums of squared difference between each weight(vector) and the mean across that col and dividing it over the total vector dimensions.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TD6ze3sZ_p_g"
      },
      "outputs": [],
      "source": [
        "GPT2_CONFIG_124MP = {\n",
        "    \"vocab_size\": 50257,\n",
        "    \"context_length\": 256,\n",
        "    \"emb_dim\": 768,\n",
        "    \"n_layers\": 12,\n",
        "    \"n_heads\": 12,\n",
        "    \"drop_rate\": 0.1,\n",
        "    \"qkv_bias\": False\n",
        "}\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, emb_dim):\n",
        "    super().__init__()\n",
        "    self.eps = 1e-5\n",
        "    self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "    self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(dim=-1, keepdim = True)\n",
        "    var = x.var(dim=-1, keepdim = True, unbiased = False)\n",
        "    norm_x = x - mean / torch.sqrt(var + self.eps)\n",
        "    return self.scale * norm_x + self.shift"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bN_Radgs21bb"
      },
      "source": [
        "GELU (Gaussian Error Linear Unit): This function basically smoothens the outputs of the each Linear Layer (neural network with hidden weights). This step is done every time after getting the outputs from the Linear Layer. This is a non-linear fucntion which solves the **dead neuron** problem (learning doesn't get hindered)  and **differentiability** of the output vectors of values a little less than 0 (as they dont automatically get changed to 0 like in RELU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "UPbplnqd2yIw"
      },
      "outputs": [],
      "source": [
        "class GELU(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0/torch.pi)) * (x + 0.044715 * torch.pow(x, 3))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lASPiUct9cpn"
      },
      "source": [
        "Alternatively, we can use the inbuilt torch.nn.GELU(x) function from pytorch library which uses the same underlying formula."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRbnh6wj-IH_"
      },
      "source": [
        "Feed Forward Layer (Linear Layer): This layer basically projects the tokens each having 768 dimensional vectors into a higher dimensional space (4 times higher) to make sure the vectors absorb richer context between the tokens and then contracted back into 768 dimensional vectors. We make sure to apply GELU after the expansion (before the contraction)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5jei-7dq9b54"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
        "        GELU(),\n",
        "        nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sm4foXU9AgsX"
      },
      "source": [
        "# Designing and instantiating the final Transformer block (including shortcut connections)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmcMAZkyA06M"
      },
      "source": [
        "Shortcut connections are added to solve the** vanishing gradient problem** (after each iteration of backpropagation, the gradient gets reduced to such extent that the learning becomes stagnant as the model assumes no difference in the gradient). To solve this, we add the previous input layer to the output layer **after the dropouts** in the Multihead attention and Feed forward mechanisms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "EmDHUP65ATgY"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.attn = MultiHeadAttention(\n",
        "        d_in = cfg[\"emb_dim\"],\n",
        "        d_out = cfg[\"emb_dim\"],\n",
        "        context_length = cfg[\"context_length\"],\n",
        "        num_heads = cfg[\"n_heads\"],\n",
        "        dropout = cfg[\"drop_rate\"],\n",
        "        qkv_bias= cfg[\"qkv_bias\"])\n",
        "    self.ff = FeedForward(cfg)\n",
        "    self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "    self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "    self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Shortcut connection for the attention block\n",
        "    shortcut = x\n",
        "    x = self.norm1(x)\n",
        "    x = self.attn(x)\n",
        "    x = self.drop_shortcut(x)\n",
        "    x = shortcut + x\n",
        "\n",
        "    # Shortcut connection for the feedforward block\n",
        "    shortcut = x\n",
        "    x = self.norm2(x)\n",
        "    x = self.ff(x)\n",
        "    x = self.drop_shortcut(x)\n",
        "    x = shortcut + x\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvC_liQgE1TS",
        "outputId": "0f0e5867-0862-4f91-c76a-048e58383c90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input Tensor shape:  torch.Size([2, 4, 768])\n",
            "Output Tensor shape:  torch.Size([2, 4, 768])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "x = torch.rand(2, 4, 768) #(batch, tokens, dimensions)\n",
        "block = TransformerBlock(GPT2_CONFIG_124MP)\n",
        "output = block(x)\n",
        "print(\"Input Tensor shape: \", x.shape)\n",
        "print(\"Output Tensor shape: \", output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvOdWacXHURl"
      },
      "source": [
        "As we can observe that the dimensions are **consistent** which is the main thing in Transformer. This is super useful so as to easily stack the tensors one on top of another without worrying too much about the dimensions. It also allows for the computation of matrix multiplications in the attention mechanism( Consider 2 matrices p x q and r x s. Here, matrix multiplication is possible only if q=r and the resultant matrix dimension would be p x s)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGmnpZc4vXnL"
      },
      "source": [
        "# Building/Connecting the entire 124M parametric model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2urQVLaPGHPI"
      },
      "outputs": [],
      "source": [
        "class GPTModel(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"]) # Initializing tok_emb\n",
        "    self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"]) # Initializing pos_emb\n",
        "    self.drop_emb = nn.Dropout(cfg[\"drop_rate\"]) # Initializing drop out rate (drop_emb)\n",
        "\n",
        "    self.trf_blocks = nn.Sequential(* [TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "    # Chain the transformer and stack the tensors till n_layers (12 in GPT2 as defined in the dict)\n",
        "\n",
        "    self.final_norm = LayerNorm(cfg[\"emb_dim\"]) # Initializing Final normalization layer\n",
        "\n",
        "    self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias = False)\n",
        "    # Initializing the final output layer containing logits.\n",
        "    # Input dim of tokens = 768(emb_dim) x N(num_tokens).\n",
        "    # Dim of the output linear layer = N(num_tokens) x 50257(vocab_size).\n",
        "    # Output dim of Logits = n_batches x 768(emb_dim) x 50257(vocab_size).\n",
        "\n",
        "  def forward(self, in_idx):\n",
        "    batch_size, seq_len = in_idx.shape # To define the shape(dimensions) of entire token_ids (in_idx)\n",
        "    tok_embeds = self.tok_emb(in_idx)  # To create token embeddings for all the token_ids (in_idx)\n",
        "    pos_embeds = self.pos_emb(torch.arange(seq_len, device = in_idx.device)) # To create pos_emb only till seq_len and start=0 is assumed\n",
        "    x = tok_embeds + pos_embeds # Combining both tok_emb and pos_emb to form the input embeddings\n",
        "    x = self.drop_emb(x) # Performing dropout\n",
        "    x = self.trf_blocks(x) # Going inside the transformers\n",
        "    x = self.final_norm(x) # Performing the final normalization\n",
        "    logits = self.out_head(x) # Passing the normalized values through linear layer to produce logits\n",
        "\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eWMY04q67Nv"
      },
      "source": [
        "Logits contain the likelihood (probability but not exactly) of all tokens from the vocab_size. Our job is to select the highest valued logits which will be essentially be the next token. To do this, its best practice to pass these logits into a non-linear function (softmax preferably) which gives the probabilities for all tokens to be the next token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ws2IIwYB64QR",
        "outputId": "a21203bc-fe0b-410a-915a-3e6e5eceb333"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input Batch:\n",
            " tensor([[15742, 11036, 24681,  8041],\n",
            "        [37378,  3791, 20331, 32351]])\n",
            "\n",
            " Output shape: torch.Size([2, 4, 50257])\n",
            "tensor([[[ 1.7085, -2.6334, -4.3429,  ...,  6.2408, -9.0471, -1.1634],\n",
            "         [ 6.4276,  0.9763, -1.3143,  ...,  2.1716,  2.4320, -2.2929],\n",
            "         [ 1.0077,  0.7788, -3.4075,  ..., -3.7775, -1.4718, -0.9255],\n",
            "         [ 3.7655, -2.8953,  0.1282,  ...,  5.6484,  1.0812,  0.8284]],\n",
            "\n",
            "        [[ 1.5700,  2.3426, -1.7597,  ..., -1.2794,  4.5621,  3.7844],\n",
            "         [ 1.5247, -2.1226, -8.3479,  ...,  2.2575,  0.3201,  2.4592],\n",
            "         [-0.4322,  8.3325, -4.1975,  ...,  1.2495, -5.7797, -1.5484],\n",
            "         [ 4.8320,  7.4809, -0.1386,  ..., -1.5529, -4.8710, -1.5130]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "batch = torch.randint(0, 50257, (2, 4)) # random token_ids from the vocab (50257) and creating a tensor of size 2(batch_size) x 4(seq_len)\n",
        "model = GPTModel(GPT2_CONFIG_124MP)\n",
        "out = model(batch)\n",
        "print(\"Input Batch:\\n\", batch)\n",
        "print(\"\\n Output shape:\", out.shape)\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPuWycNlIiRj",
        "outputId": "1a57d8e7-d5d3-4a47-ec8d-47ee9d27bcd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of parameters: 162419712\n"
          ]
        }
      ],
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters: {total_params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vh2dy3EVb4ST"
      },
      "source": [
        "We can observe that our model has 162M parameters which is more than 124M GPT2 model. This is because the GPT2 model does something called **Weight Tying** which is basically reusing the weights of one layer (Token embedding layer) as another layer (Linear output layer) where both layers have the dimensions 50257 x 768.\n",
        "\n",
        "GPT does this to reduce the **model size** and **computational complexity**.\n",
        "\n",
        "However, for training the weights/parameters, its better to have more parameters and not resuse any weights as it can capture more complex relationships which increses the model's learning rate and makes them better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0uS27ccmgG9"
      },
      "source": [
        "# Generating text from Output Tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNveJbpWnCKB"
      },
      "source": [
        "We build a function which generates tokens by taking in model(GPTModel), idx(basically input having dimension as: batch x n_tokens), max_new_tokens(how many new tokens to generate), context_size (how many tokens to look at/ consider while predicting the next token)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "xfstnCAkS0Sa"
      },
      "outputs": [],
      "source": [
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "  for _ in range(max_new_tokens):\n",
        "\n",
        "    idx_cond = idx[:, -context_size:]\n",
        "    # Consider example where idx = [[1, 2, 3, 4 ,5, 6, 7, 8],[9, 10, 11, 12, 13, 14]].\n",
        "    # If we give context_size as 5 (lets say) which means the LLM can only look at 5 token_ids at once from each batch to make the prediction.\n",
        "    # So, we only consider the last 5 token_ids from each batch. Thats why the -context_size.\n",
        "    # Therefore, the token_ids [[4, 5, 6, 7, 8], [10, 11, 12, 13, 14]]\n",
        "    # which are basically the last 5 token_ids in their respective batches that are to be considered.\n",
        "\n",
        "    with torch.no_grad():\n",
        "      logits = model(idx_cond)  # To get the output logits which has the dim (batch, n_tokens, vocab_size)\n",
        "\n",
        "    logits = logits[:, -1, :]\n",
        "    # Only the last output logit vector is to be considered. So we dont have to look at all tokens.\n",
        "    # Hence, dimensions of logits will also get changed from (batch, n_tokens, vocab_size) -> (batch, vocab_size)\n",
        "\n",
        "    probas = torch.softmax(logits, dim=1)\n",
        "    # Applying softmax to obtain probabilities of becoming the next token for all tokens in the vocab_size.\n",
        "    # Dimensions after softmax will still be (batch, vocab_size).\n",
        "\n",
        "\n",
        "    idx_next = torch.argmax(probas, dim=1, keepdim=True)\n",
        "    # To get the token_id of the largest probability value from the vocab_entry.\n",
        "\n",
        "    idx = torch.cat((idx, idx_next), dim=1)\n",
        "    # Append to the newly found token_id to the running sequence. Dim = (batch, n_tokens+1)\n",
        "\n",
        "  return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdN9sSjLu7qT"
      },
      "source": [
        "Although, we could have just implemented the torch.argmax() function directly to find the largest probability value, implementing softmax() is crucial if we want to introduce Temperature Scaling which basically induces creativity in the model by not selecting high probalility value all the time but to mix and match at times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DADdUDQ_unG9",
        "outputId": "5b3b7315-b585-498e-c25f-14d004a74285"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoded Token IDs: [37316, 318, 2826, 1022]\n",
            "\n",
            " Encoded Tensor Shape: torch.Size([1, 4])\n"
          ]
        }
      ],
      "source": [
        "start_context = \"Football is played between\"\n",
        "encoded = tokenizer.encode(start_context)\n",
        "print(\"Encoded Token IDs:\", encoded)\n",
        "encoded_tensor = torch.tensor(encoded).unsqueeze(0) #Unsqueeze adds a singleton dimension at index = 0\n",
        "print(\"\\n Encoded Tensor Shape:\", encoded_tensor.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AukCXsZWwbIU"
      },
      "source": [
        "We will use model.eval() to disable components like dropout since we are not training the model at the moment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbHffS-5wG8H",
        "outputId": "5af981b3-49d9-4a06-cb75-80f3a8662fb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output: tensor([[37316,   318,  2826,  1022,  4633, 11462, 38946, 45935, 12298, 19812]])\n",
            "\n",
            " Output Length: 10\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "out = generate_text_simple(\n",
        "      model = model,\n",
        "      idx = encoded_tensor,\n",
        "      max_new_tokens = 6,\n",
        "      context_size = GPT2_CONFIG_124MP[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(\"Output:\", out)\n",
        "print(\"\\n Output Length:\", len(out[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2yLs0XjxzdR"
      },
      "source": [
        "As we can see upon giving just 4 input tokens, the model was able to generate 6 more (total 10) token ids by undergoing all the mentioned steps. Now, we will decode to see if the newly generated token_ids make sense."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMRU-5WqxvR5",
        "outputId": "66dcd226-fb54-485d-de10-04e9ee338148"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Football is played between negative guaranteed Barbarian delusionalAW fictional\n"
          ]
        }
      ],
      "source": [
        "decode_text = tokenizer.decode(out.squeeze(0).tolist()) # Squeeze removes a singleton dimension at index = 0\n",
        "print(decode_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSAFFqD300Uq"
      },
      "source": [
        "As we can see, the model produced gibberish output tokens because we haven't **TRAINED** the model yet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUseQ2tg80RR"
      },
      "source": [
        "# Calculating **Loss function**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lYFLEOt_OmQ"
      },
      "source": [
        "We load our verdict.txt dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "KYkqykBFzqwS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "file_path = \"/content/the-verdict.txt\"\n",
        "# url = \" \"  We can also mention the URL inside the quotes to access the dataset\n",
        "\n",
        "# if not os.path.exists(file_path):\n",
        "#     with url.lib.request.urlopen(url) as response:\n",
        "#         text_data = response.read().decode('utf-8')\n",
        "#     with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "#         file.write(text_data)\n",
        "# else:\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    text_data = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Der93qda_UOp",
        "outputId": "d543e7c5-8800-4b29-9f22-9e46f54da447"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Characters:  20479\n",
            "Total Tokens:  5145\n"
          ]
        }
      ],
      "source": [
        "total_characters = len(text_data)\n",
        "total_tokens = len(tokenizer.encode(text_data))\n",
        "print(\"Total Characters: \", total_characters)\n",
        "print(\"Total Tokens: \", total_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4cCJofeBJg1"
      },
      "source": [
        "Spliting the tokens for **training** (90%) and **validation** (10%)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "DwRcrlZ__0Bu"
      },
      "outputs": [],
      "source": [
        "train_ratio = 0.9\n",
        "split_idx = int (train_ratio * len(text_data))\n",
        "train_data = text_data[:split_idx]\n",
        "val_data = text_data[split_idx:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_rizUO8CLv4"
      },
      "source": [
        "Using dataloaders to handle multiple batches efficiently as it can automatically divide the dataset into multiple batches making training easy to iterate.\n",
        "Also, context_size here is 256 compared to 1024 in GPT2. This is done so that we can generate the tokens quickly as its computationally friendly. We've also limited batch_size to 2 for the same reason."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "cPJKh7npCK7R"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_loader = create_dataloader_v1(\n",
        "              train_data, batch_size = 2, max_length = GPT2_CONFIG_124MP[\"context_length\"],\n",
        "              stride = GPT2_CONFIG_124MP[\"context_length\"], drop_last= True, shuffle=True, num_workers = 0)\n",
        "\n",
        "\n",
        "val_loader = create_dataloader_v1(\n",
        "              val_data,  batch_size = 2, max_length = GPT2_CONFIG_124MP[\"context_length\"],\n",
        "              stride = GPT2_CONFIG_124MP[\"context_length\"], drop_last= True, shuffle=True, num_workers = 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz0q8M0WE8d2"
      },
      "source": [
        "Sanity checks to cover the edge cases where the\n",
        "number of training tokens and validation tokens are **LESSER** than the context length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "tXEzOGmVFWY-"
      },
      "outputs": [],
      "source": [
        "# For training tokens\n",
        "if (total_tokens * train_ratio < GPT2_CONFIG_124MP[\"context_length\"]):\n",
        "  print(\"Not enough tokens for the train_loader.Try to lower the context_length or increase the train_ratio\")\n",
        "\n",
        "# For validation tokens\n",
        "if(total_tokens * (1 - train_ratio) < GPT2_CONFIG_124MP[\"context_length\"]):\n",
        "  print(\"Not enough tokens for the val_loader.Try to lower the context_length or decrease the train_ratio\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMreSIuzGKiC",
        "outputId": "f8ea7134-3331-4d71-ba47-e6333f552c1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loader:\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "\n",
            " Validation Loader:\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n"
          ]
        }
      ],
      "source": [
        "print(\"Train Loader:\")\n",
        "for x, y in train_loader:\n",
        "  print(x.shape, y.shape)\n",
        "\n",
        "print(\"\\n Validation Loader:\")\n",
        "for x, y in val_loader:\n",
        "  print(x.shape, y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YItzhU9LJR_D"
      },
      "source": [
        "As we can we the data is split in 90-10 format (9 training batches and 1 validation batch). Here 2 represents batch_size. So, each batch of training and validation is computing 2 rows of 256 input tokens each.\n",
        "Example: Lets assume in this example, the context_length = 4, stride = 4 and batch_size = 2.\n",
        "So, input_tokens =\n",
        "[[234, 4558, 4589, 7897], [7272, 9375, 12345, 45678]]\n",
        "and\n",
        "target_tokens = [[4558, 4589, 7897, 38898], [9375, 12345, 45678, 5381]]\n",
        "Here, target_tokens represent the correct desired output token that we want the model to get. But since it's not trained yet, it wont produce the desired output. Also, for each batch(row) in input_token there are 4 steps going on. For the first input token [234], the desired target_tokens is at the first index in the target_tokens [4558]. In 2nd iteration for the same batch, first two input tokens get considered [234, 4558] and the target_tokenz value will be in the second index [4589]. Lastly, during 4th iteration, while finising the first batch, the input_tokens will have considered the first 4 tokens [234, 4558, 4589, 7897] and the target_tokens will have [38898] which is what we want the model to generate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "zBtnG_xaJJr4"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT2_CONFIG_124MP) #model instance\n",
        "model.eval(); # To disable dropouts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u0I9c7fR1vR"
      },
      "source": [
        "The transformer basically takes in the input_tokens to produce logits. These logits have dimensions [2(batch_size), 256(context_size), 50257(vocab_size)]. However, we need to flatten these logits dim from 3D to 2D. So, we use logits.flatten(0,1) to flatten them across 1st and 2nd dim. Hence, the resultant dim of the logits will be [(512(tokens) x 50257(vocab_size))]. We should also do the same flattening for the target_tokens too.\n",
        "\n",
        "Next, we need to calculate the loss function which is conventionally done by taking the (512 x 50257) logits and passing it through a softmax activation function which basically finds the highest token probabilaties across columns(vocab_size) for each row(input_token). Without training, these probabilities wont match with the right target_tokens. To solve this problem, we need to take the target_tokens ids and look up their probabilities respectively in the vocab_size. By doing that we will get another set of probabilities [p11, p12, p13, p14....p1256, p21, p22...p2256]. These values represent probabilities that the corresponding target_tokens appear after each input_token iteration. If the model is trained well, then each of the probability values will be closer to 1. Since that is not the case, the values are not closer to 1. To actually calculate the cross-entropy loss, we need to compute:\n",
        " -( log likelihood or average of sum of log of probabilities of the target_tokens for each batch).  \n",
        " Eg: For batch 1, the loss would be -(logp11 + logp12 + ... logp1256) / 256.\n",
        " Similarly for batch 2, the loss would be -(logp21 + logp22 + ....logp2256) / 256.\n",
        " Finally, we ADD both batch loss to calculate the overall cross-entropy loss.\n",
        " Note: We take negative log likelihood instead of positive because we need to find the local minima and would want our loss to minimize after each iteration. If we take postive log likelihood then we would need to maximize our loss to find the best model. Hence we take -log likelihoood.\n",
        "\n",
        " Instead of doing all above procedures, pytorch lib has a special function torch.nn.functional.cross_entropy() which takes in the flattened logits (prodced by the transformer) and the target_tokens as its parameters and returns the loss, while undergoing all the above steps(softmax + negative log likelihood) within the function itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "3FAMyCotRjL-"
      },
      "outputs": [],
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "  input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "  logits = model(input_batch)\n",
        "  loss = torch.nn.functional.cross_entropy(logits.flatten(0,1), target_batch.flatten())\n",
        "  return loss\n",
        "\n",
        "# To calcualate loss for all batches,\n",
        "# we implement another function which takes in whole data_loader containing multiple batches.\n",
        "\n",
        "def calc_loss_loader(data_loader, model, device, num_batches = None):\n",
        "  total_loss = 0.\n",
        "  if len(data_loader) == 0:    # If the dataloader contains nothing\n",
        "    return float(\"nan\")\n",
        "  elif num_batches is None:    # If num_batches is not explicitly given as an argument, then the dataloader considers len(dataloader) which are the previously split batches (9 for training and 1 for validation)\n",
        "    num_batches = len(data_loader)\n",
        "  else:   # If num_batches > len(dataloader), then it takes the minimum value between them in order to reduce the num_batches\n",
        "    num_batches = min(num_batches, len(data_loader))\n",
        "\n",
        "  for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "    if i < num_batches:\n",
        "      loss = calc_loss_batch(input_batch, target_batch, model, device) # Calculate loss for every batch\n",
        "      total_loss += loss.item() # Add those loss and store it in total_loss variable.\n",
        "                                # .item() is used to convert single element tensor into a scalar type like float or int\n",
        "                                # so, that we can compute mean loss across all batches\n",
        "\n",
        "    else:\n",
        "      break\n",
        "\n",
        "  return total_loss / num_batches # Returning the average/mean loss across all batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gyd2ZAwwoLdR",
        "outputId": "d7c086cd-b5cf-4813-98ed-3d6dc8c8a825"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss:  0.4904715220133464\n",
            "Validation Loss:  7.583980083465576\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cpu\") # Using device setting we can ensure that the data is loaded onto the same device as LLM model\n",
        "model.to(device)\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "with torch.no_grad(): # Because we haven't started training yet, so we can disable gradient tracking\n",
        "  train_loss = calc_loss_loader(train_loader, model, device)\n",
        "  val_loss = calc_loss_loader(val_loader, model, device)\n",
        "\n",
        "print(\"Training Loss: \",train_loss)\n",
        "print(\"Validation Loss: \",val_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxyQOOYWtv9g"
      },
      "source": [
        "We find both training and validation loss to be 11.47. These are the values that should be minimized using backpropagation which updates the weights thus increasing model's accuracy and thereby decreasing overall cost function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TjD9z-5wV5k"
      },
      "source": [
        "# Backpropagation and Adam optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgULHW1dN-2N"
      },
      "source": [
        "We use loss.backward() function which directly computes the loss gradients(Cost function) which is basically change in loss function W.R.T all the weights. Our goal is to update weights by reducing the cost function. Simple formula for this weights update can be W = W + α. ∂Co/∂W\n",
        "\n",
        "Lets assume, we have one output neuron with activation a(L) which is connected to a previous neuron with activation a(L-1) via the weights w(L).\n",
        "Now, we want the output neuron to have a desired value of y = 1 but that is generally not the case before training. So, we can calculate the cost function, Co = (a(L) - y)^2\n",
        "\n",
        "Also, a(L) = w(L)*a(L-1) + b(L)\n",
        "Here, computing ∂Co/∂W involves implementing chain rule which basically takes the partial derivative of current cost function wrt previous weights:\n",
        "∂Co/∂W = (∂(a(L))/∂W) * (∂Co/∂(a(L)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDc6_AyeT9Nl"
      },
      "source": [
        "Finally, after computing the cost function (gradients) wrt all weights, we update the weights using optimizers like Adam and Stochastic Gradient.\n",
        "Adam is better than Stochastic Gradient as we dont need to update the learning rate manually each time we get the cost function. Adam does it automatically.\n",
        "However, for deep learning models, even Adam optimizer performs the L2 regularization incorrectly(A.K.A weight decay which basically penalizes large weights by adding a loss term to the cost function) and this leads to overfitting in deep learning models.\n",
        "So, models like BERT and GPT use **AdamW** which basically does the weight decay and arrives at the convergence in the loss landscape very quickly within a few number of steps by also preventing overfitting or overshooting of data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWKplwYYfZpi"
      },
      "source": [
        "# Final Training Loop implementation using all of the classes, functions and methodologies used earlier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAm22IDbiJOG"
      },
      "source": [
        "We can finally create a holistic training loop which uses all the above functions to pre-train the LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhpYQ8T4YavC",
        "outputId": "9547bc7f-4b8c-4db8-a6fe-e3097d858e16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
            "tiktoken version: 0.9.0\n"
          ]
        }
      ],
      "source": [
        "! pip3 install tiktoken\n",
        "import importlib\n",
        "import tiktoken\n",
        "\n",
        "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "6aelryKogE_A"
      },
      "outputs": [],
      "source": [
        "def text_to_token_ids(text, tokenizer):   #Uses tokenizer to encode text to tokens\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
        "    return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):   #Uses tokenizer to decode tokenids back to text\n",
        "    flat = token_ids.squeeze(0) # remove batch dimension\n",
        "    return tokenizer.decode(flat.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ZxoTUvN3iZjQ"
      },
      "outputs": [],
      "source": [
        "GPT2_CONFIG_124MP = {\n",
        "    \"vocab_size\": 50257,\n",
        "    \"context_length\": 256,\n",
        "    \"emb_dim\": 768,\n",
        "    \"n_layers\": 12,\n",
        "    \"n_heads\": 12,\n",
        "    \"drop_rate\": 0.1,\n",
        "    \"qkv_bias\": False\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "XVy1EbBjYx_4"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "0MfEFtdYY0uX"
      },
      "outputs": [],
      "source": [
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True,\n",
        "                         num_workers=0):\n",
        "\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "hpNJ7761Zh-4"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (d_out % num_heads == 0), \\\n",
        "            \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.triu(torch.ones(context_length, context_length),\n",
        "                       diagonal=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "\n",
        "        # Original mask truncated to the number of tokens and converted to boolean\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        # Use the mask to fill attention scores\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec) # optional projection\n",
        "\n",
        "        return context_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "HqgfoCt3a2Uy"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.scale * norm_x + self.shift\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), ## Expansion\n",
        "            GELU(), ## Activation\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]), ## Contraction\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "CN4s2pIta3xq"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"])\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shortcut connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        # Shortcut connection for feed forward block\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        # 2*4*768\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        return x\n",
        "        # 2*4*768"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "GLyeq8VJbRk5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(\n",
        "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT2_CONFIG_124MP)\n",
        "model.eval();  # Disable dropout during inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "stU1BmUkbkGR"
      },
      "outputs": [],
      "source": [
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "    # idx is (batch, n_tokens) array of indices in the current context\n",
        "\n",
        "    ###Input batch eg:\n",
        " ###tensor([[6109, 3626, 6100,  345],\n",
        "        ##[6109, 1110, 6622,  257]])\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "        # Crop current context if it exceeds the supported context size\n",
        "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
        "        # then only the last 5 tokens are used as context\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "\n",
        "        # Get the predictions\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond) ### batch, n_tokens, vocab_size\n",
        "\n",
        "        # Focus only on the last time step\n",
        "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # Apply softmax to get probabilities\n",
        "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
        "\n",
        "        # Get the idx of the vocab entry with the highest probability value\n",
        "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
        "\n",
        "        # Append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
        "\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Igzps4l-cY2F"
      },
      "outputs": [],
      "source": [
        "train_ratio = 0.90\n",
        "split_idx = int(train_ratio * len(text_data))\n",
        "train_data = text_data[:split_idx]\n",
        "val_data = text_data[split_idx:]\n",
        "\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_loader = create_dataloader_v1(\n",
        "    train_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT2_CONFIG_124MP[\"context_length\"],\n",
        "    stride=GPT2_CONFIG_124MP[\"context_length\"],\n",
        "    drop_last=True,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_loader = create_dataloader_v1(\n",
        "    val_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT2_CONFIG_124MP[\"context_length\"],\n",
        "    stride=GPT2_CONFIG_124MP[\"context_length\"],\n",
        "    drop_last=False,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g25giRELc7j-",
        "outputId": "3b6591c1-7f1d-4fbb-dbf7-c004e701b2f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loader:\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "\n",
            "Validation loader:\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "9\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "print(\"Train loader:\")\n",
        "for x, y in train_loader:\n",
        "    print(x.shape, y.shape)\n",
        "\n",
        "print(\"\\nValidation loader:\")\n",
        "for x, y in val_loader:\n",
        "    print(x.shape, y.shape)\n",
        "\n",
        "print(len(train_loader))\n",
        "print(len(val_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LO6WTqphdAo4",
        "outputId": "72a65049-5e84-4922-d755-1b00347aa885"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training tokens: 4608\n",
            "Validation tokens: 512\n",
            "All tokens: 5120\n"
          ]
        }
      ],
      "source": [
        "train_tokens = 0\n",
        "for input_batch, target_batch in train_loader:\n",
        "    train_tokens += input_batch.numel()\n",
        "\n",
        "val_tokens = 0\n",
        "for input_batch, target_batch in val_loader:\n",
        "    val_tokens += input_batch.numel()\n",
        "\n",
        "print(\"Training tokens:\", train_tokens)\n",
        "print(\"Validation tokens:\", val_tokens)\n",
        "print(\"All tokens:\", train_tokens + val_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkxPBht8e8i7"
      },
      "source": [
        "We consider and calculate cross entropy loss instead of normal loss because cross entropy loss takes the negative average of log of probabilities (which has wider range and transform small probabilities into large negative values, making it easier for the optimizer to distinguish between wrong and right predictions). This is better than just calculating loss over raw probabilities (which are bound from 0 to 1 and the gradients would be small and as a result, slow down learning.).\n",
        "\n",
        "However, instead of using the whole holistic process above, we can us a pytorch function torch.nn.fucntional.cross_entropy() to calculate cross entroy loss for both training and validation batches of tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "SbyfGCe0dpVE"
      },
      "outputs": [],
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "    return loss\n",
        "\n",
        "\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        # Reduce the number of batches to match the total number of batches in the data loader\n",
        "        # if num_batches exceeds the number of batches in the data loader\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdUWPN9Qdvwg",
        "outputId": "9e2d4e8f-7b52-4176-a84a-3d52f0b2b061"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device.\n",
            "Training loss: 11.47785112592909\n",
            "Validation loss: 11.477570533752441\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Select device (Ensure compatibility with Google Colab, Apple MPS, or CPU)\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")  # Use CUDA if available\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")  # Use Apple Metal Performance Shader (for Apple Silicon)\n",
        "else:\n",
        "    device = torch.device(\"cpu\")  # Default to CPU\n",
        "\n",
        "print(f\"Using {device} device.\")\n",
        "\n",
        "# Move the model to the selected device\n",
        "model.to(device)  # No assignment needed\n",
        "\n",
        "# Set manual seed for reproducibility\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# Ensure loss computation is done on the same device\n",
        "with torch.no_grad():  # Disable gradient tracking for efficiency\n",
        "    train_loss = calc_loss_loader(train_loader, model, device)  # Ensure train_loader is on the correct device\n",
        "    val_loss = calc_loss_loader(val_loader, model, device)  # Ensure val_loader is on the correct device\n",
        "\n",
        "print(\"Training loss:\", train_loss)\n",
        "print(\"Validation loss:\", val_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "hz2aCxkkp57q"
      },
      "outputs": [],
      "source": [
        "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                       eval_freq, eval_iter, start_context, tokenizer):\n",
        "    # Initialize lists to track losses and tokens seen\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "\n",
        "            loss.backward() # Calculates loss gradients using backproparation. MOST IMPORTANT step in order to update weights.\n",
        "\n",
        "            optimizer.step() # Update model weights using loss gradients\n",
        "            tokens_seen += input_batch.numel() # Returns the total number of elements (or tokens) in the input_batch.\n",
        "            global_step += 1\n",
        "\n",
        "            # Optional evaluation step\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # Print a sample text after each epoch\n",
        "        generate_and_print_sample(\n",
        "            model, tokenizer, device, start_context\n",
        "        )\n",
        "\n",
        "    return train_losses, val_losses, track_tokens_seen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "fDvWNy54eP8x"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter): # Function to return train_loss and val_loss\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "5tnm37YBeGPW"
      },
      "outputs": [],
      "source": [
        "def generate_and_print_sample(model, tokenizer, device, start_context): #Function which prints the output after each epoch\n",
        "    model.eval()\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "    with torch.no_grad():\n",
        "        token_ids = generate_text_simple(\n",
        "            model=model, idx=encoded,\n",
        "            max_new_tokens=50, context_size=context_size\n",
        "        )\n",
        "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
        "    model.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Un8xg9_HJPsp"
      },
      "source": [
        "Creating an instance to run our pre-trained LLM loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ao0Sxk3h6e7d",
        "outputId": "cb018300-e70f-4c92-d328-b29050b77405"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Step 000000): Train loss 51.396, Val loss 49.269\n",
            "Ep 1 (Step 000005): Train loss 11.200, Val loss 11.886\n",
            "Every effort moves you of the � his of the his of the his of the of the his of the his the his of the the of the his the of the the his of the his of the Zone the his of the of the . . . . . . .\n",
            "Ep 2 (Step 000010): Train loss 13.548, Val loss 14.670\n",
            "Ep 2 (Step 000015): Train loss 7.698, Val loss 8.921\n",
            "Every effort moves you my about my my and my my my my not can my my my my not not not at my's for the of my for him, and I was not at my and at my my and's and my my myed at my not to have\n",
            "Ep 3 (Step 000020): Train loss 6.421, Val loss 7.992\n",
            "Ep 3 (Step 000025): Train loss 7.349, Val loss 9.388\n",
            "Every effort moves you \"as: \"as: \" had been, but \" had have!! \"as \" \" \" \"as! \"as! \", when, youas! you, had have! \" \" \"as: \" \" \" had! The\n",
            "Ep 4 (Step 000030): Train loss 4.937, Val loss 7.540\n",
            "Ep 4 (Step 000035): Train loss 5.597, Val loss 8.226\n",
            "Every effort moves you; and into a have was his glory after his part a by his last is not with a smile's note J central's between his pictures: \" himself his with a himself at the donkey bes; and into into a little between is show bes neg\n",
            "Ep 5 (Step 000040): Train loss 3.914, Val loss 6.959\n",
            "Every effort moves you know it to me to put it to it to me to me--I turned not to me an was his! The to put it to me it was not to me to put it happened put it to wander up to put it to me--I\n",
            "Ep 6 (Step 000045): Train loss 3.661, Val loss 7.035\n",
            "Ep 6 (Step 000050): Train loss 2.900, Val loss 6.710\n",
            "Every effort moves you in the inevitable, and pushed one of the picture, struck. \"Never.  \", and herself, I felt him, in the moment, I looked, at my elbow and continued to wander, and down, at the picture was\n",
            "Ep 7 (Step 000055): Train loss 2.593, Val loss 6.641\n",
            "Ep 7 (Step 000060): Train loss 1.984, Val loss 6.657\n",
            "Every effort moves you?\"  \"Yes, and he was when I don. \"Oh, so disarming, and herself, I felt to see. \"Oh, I saw, one might put it, married her when I felt, I was his\n",
            "Ep 8 (Step 000065): Train loss 1.499, Val loss 6.618\n",
            "Ep 8 (Step 000070): Train loss 1.159, Val loss 6.701\n",
            "Every effort moves you?\"  \"Yes, and he was one of his pictures me. \"Once, when I looked up, I seemed to see a smile behind his close grayish beard, and his painting, married, and down, at the end of\n",
            "Ep 9 (Step 000075): Train loss 0.909, Val loss 6.763\n",
            "Ep 9 (Step 000080): Train loss 0.611, Val loss 6.905\n",
            "Every effort moves you?\"  \"Yes, and he was one of a good-t--so, till nearly a year after me, becoming the man of the moment--as Jack himself, one might put it, a show, you know. He says they\n",
            "Ep 10 (Step 000085): Train loss 0.424, Val loss 7.045\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Training completed in 0.61 minutes.\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT2_CONFIG_124MP)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
        "\n",
        "num_epochs = 10\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
        "    start_context=\"Every effort moves you\", tokenizer=tokenizer)\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCNGlzV3jfhZ"
      },
      "source": [
        "We can see from the above text that the loss is getting minimized which is good. But the generated words still dont fit the context as it's trained using only one file having around 50000 tokens. We can also see that the model is straight up copying(memorizing) from the txt.file. This is the clssic case of overfitting. Finally, we can see that the validation loss after 2nd epoch is almost constant with very little change compared to the training loss which is going down after every epoch. This is again an example of overfitting and that the model is capable of only learning the specific patterns(training) only and not generalizable patterns (validation). So, even though our training loss is very less, doesn't mean the model is capable of recognizing new patterns which it hasn't seen before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "w66ImvtH_CA4",
        "outputId": "c4461d0a-9d43-4f0f-c4c0-37853945299e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "\n",
        "\n",
        "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
        "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "\n",
        "    # Plot training and validation loss against epochs\n",
        "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
        "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    ax1.legend(loc=\"upper right\")\n",
        "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
        "\n",
        "    # Create a second x-axis for tokens seen\n",
        "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
        "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
        "    ax2.set_xlabel(\"Tokens seen\")\n",
        "\n",
        "    fig.tight_layout()  # Adjust layout to make room\n",
        "    plt.savefig(\"loss-plot.pdf\")\n",
        "    plt.show()\n",
        "\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "co5CnNUanF-r"
      },
      "source": [
        "# Decoding strategies (temperature scaling and k-sampling)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHX8WwEq_8eX"
      },
      "source": [
        "**Temperature Scaling**:\n",
        "In order for the model to perform better, we introduce temperature scaling which is basically a constant value (T>0) that's added into to the softmax function and this value divides all the output logits. There are 3 possible outcomes based on the value of T:\n",
        "If T>0 and <1 , this means that the difference between the largest output logit and all other small output logits will be huge.\n",
        "If T>>>>1, then there wont be much difference between the correct output logits with the other ones.\n",
        "So, one of the main goals for generating useful and creative text and not just a bunch of gibberish is actually selecting the right value for T.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7dzzZgtCuPV"
      },
      "source": [
        "**Multinomial Distribution**: We also utilize the multinomial distribuion function from the pytorch library which bsaically doesn't just select the highest valued logit but it will select the logits from a multnomial distribution curve. So, chances are, every now and then it will also select unlikely values from the curve."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sum2EYlbCJQc"
      },
      "source": [
        "**K-sampling**: Very rarely, the multinomial fucntion can also select a few outputs which are super unlikely and have no relevance to the context whatsoever. To avoid this, we can use K-sampling where we can provide 'K' value (integer) and the model considers only those many logits from the top. Rest of them are set to -inf so that after passing them through softmax, they become zero.\n",
        "Example: Lets assume our ouput logits are: [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]. Lets say k=3\n",
        "After applying the k-sampling, the top-k (3 in this case) logits that will be displayed are: ([6.7500, 6.2800, 4.5100]). Rest are 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "sZEp-oqoFu-S"
      },
      "outputs": [],
      "source": [
        "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
        "\n",
        "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # New: Filter logits with top_k sampling\n",
        "        if top_k is not None:\n",
        "            # Keep only top_k values\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1]\n",
        "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
        "\n",
        "        # New: Apply temperature scaling\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
        "\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
        "\n",
        "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
        "\n",
        "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
        "            break\n",
        "\n",
        "        # Same as before: append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
        "\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOmmX9WdF3wY",
        "outputId": "63177d49-c7f1-4df5-9ad0-e6edddcf605b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " Cristiano Ronaldo plays for, if't sayquite he to face the drawing-room--so of\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Ensure device selection\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move model to the correct device\n",
        "model.to(device)\n",
        "\n",
        "# Convert text to token IDs and move them to the same device\n",
        "idx = text_to_token_ids(\"Cristiano Ronaldo plays for\", tokenizer).to(device)\n",
        "\n",
        "# Generate output while ensuring all tensors are on the same device\n",
        "with torch.no_grad():  # Disable gradients for efficiency. Since we are not updating the weights, we can disable the gradients\n",
        "    token_ids = generate(\n",
        "        model=model,\n",
        "        idx=idx,  # Make sure idx is on the same device\n",
        "        max_new_tokens=15,\n",
        "        context_size=GPT2_CONFIG_124MP[\"context_length\"],\n",
        "        top_k=25,\n",
        "        temperature=1.4\n",
        "    )\n",
        "\n",
        "# Convert back to text\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3JHZjzmdQ-v"
      },
      "source": [
        "We can see that our model has produced more coherent text but it still is not enough to generate super meaningful text as it's trained only on 50000 tokens from 1 single source (the verdict)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2eMyqRmRIn1"
      },
      "source": [
        "To save our model (containing parameters + optimizer-AdamW), we can use torch.save() function where we can further use model.state_dict() to store them in a dictionary format. This makes our model easily sharable with all the trained parameters and optimizer. The reason behind saving even the optimizer is because they contain information about hyperparameters and historical data such as past gradients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ld0OcQslRGQF"
      },
      "outputs": [],
      "source": [
        "model = GPTModel(GPT2_CONFIG_124MP)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
        "\n",
        "torch.save({\n",
        "    \"model_state_dict\": model.state_dict(),\n",
        "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "    },\n",
        "    \"model_and_optimizer.pth\" # Where the model will be saved (.pth is a extension convention given for storing pytorch files. Alternatively, we can use any extension nomenclature).\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1ZL6kFBalAD"
      },
      "source": [
        "To load the weights and optimizer from our local system, we can use torch.load() and mention the path as a paramenter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-OcMqAe8RDRx"
      },
      "outputs": [],
      "source": [
        "checkpoint = torch.load(\"model_and_optimizer.pth\")\n",
        "model = GPTModel(GPT2_CONFIG_124MP)\n",
        "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
        "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "model.train(); #Opening in training mode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYhMKJihVCB4"
      },
      "source": [
        "# Loading pre-trained weights of GPT-2 from OPEN AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvH1pyH9LlgH"
      },
      "source": [
        "Building good LLMs are directly proportional to their sheer size in parameters. The more tokens they've been trained on, the better performance and more meaning they can capture from our input tokens. Since, we dont possess these huge resources (I'm broke) and time, we will import GPT2's pretrained weights which are open-source and integrate it with our GPT architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "oaWZqzoQQ0Qn"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow>=2.15.0 tqdm>=4.66"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYH6haVoMtz4",
        "outputId": "ee916323-855c-450c-f4f5-ab32c040deee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.18.0\n",
            "tqdm version: 4.67.1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tqdm\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"tqdm version:\", tqdm.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "aL-2a6B2l7kI"
      },
      "outputs": [],
      "source": [
        "from gpt_download3 import download_and_load_gpt2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUumnGwwml6J",
        "outputId": "5ec51832-76fa-4f5e-bb0e-69fbeb8fe156"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 171kiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "encoder.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 2.73MiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "hparams.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 140kiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "model.ckpt.data-00000-of-00001: 100%|██████████| 498M/498M [00:45<00:00, 11.0MiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "model.ckpt.index: 100%|██████████| 5.21k/5.21k [00:00<00:00, 7.23MiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "model.ckpt.meta: 100%|██████████| 471k/471k [00:00<00:00, 1.36MiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "vocab.bpe: 100%|██████████| 456k/456k [00:00<00:00, 1.52MiB/s]\n"
          ]
        }
      ],
      "source": [
        "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MClWYg-imrK0",
        "outputId": "8e8057ee-7924-49ed-8fe2-82eb081a3f6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
            "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
          ]
        }
      ],
      "source": [
        "print(\"Settings:\", settings)\n",
        "print(\"Parameter dictionary keys:\", params.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muG-E0NingkC"
      },
      "source": [
        "Here, 'blocks' refer to the weights of all the transformer blocks (multihead attention having keys, queries and value matrices, layer normalizations 1 and 2 each having both scale(multiply) and shift(add) parameters, feedfowrward network having fully connected layer(gets projected into a 4x dimensional hidden layer) and output_proj layers(dimensions are brought back matching the original)) ,\n",
        "\n",
        "'g' refers to scale parameters of the final layerNorm, 'b' refers to the shift parameters final layerNorm , 'wpe' refers to the weights of positional embedddings which have size of 1024 x 768 (equals to context_size x embedding_dim) and 'wte' refers to weights of the token embeddings of size 50257 x 768 (vocab_size x embedding_dim)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hv79d_zInZDo",
        "outputId": "0816d262-efa7-4d87-99de-7464620b426f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n",
            "   0.04531523]\n",
            " [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n",
            "   0.04318958]\n",
            " [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n",
            "  -0.08785918]\n",
            " ...\n",
            " [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n",
            "  -0.06952604]\n",
            " [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n",
            "  -0.02245961]\n",
            " [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n",
            "   0.12067825]]\n",
            "Token embedding weight tensor dimensions: (50257, 768)\n"
          ]
        }
      ],
      "source": [
        "print(params[\"wte\"])\n",
        "print(\"Token embedding weight tensor dimensions:\", params[\"wte\"].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9ypFBgdQzx_"
      },
      "source": [
        "Creating a dictionay to store all the different model configs of GPT2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "X3SQ5r7GsYhr"
      },
      "outputs": [],
      "source": [
        "# Define model configurations in a dictionary for compactness\n",
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}\n",
        "\n",
        "# Copy the base configuration and update with specific model settings\n",
        "model_name = \"gpt2-small (124M)\"  # Example model name\n",
        "NEW_CONFIG = GPT2_CONFIG_124MP.copy()\n",
        "NEW_CONFIG.update(model_configs[model_name])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhNb0bZ4RDQ1"
      },
      "source": [
        "Since our GPT architecture involved us taking context_length of only 256, while the actual GPT model was designed to take in context_length of 1024, we will update our architecture's context length accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "trJISCF7QxIw"
      },
      "outputs": [],
      "source": [
        "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
        "gpt = GPTModel(NEW_CONFIG)\n",
        "gpt.eval();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0GgT3AvSNlN"
      },
      "source": [
        "Since we had initialized our model with random weights previously, we will update them with the OPEN AI's weights which gets the weights from the 'params' dictionary. For this we will first define a small assign utility function that checks whether two tensors or arrays (left and right) have the same dimensions or shape and returns the right tensor as trainable PyTorch parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "u5aXfD-fRpeU"
      },
      "outputs": [],
      "source": [
        "def assign(left, right):\n",
        "    if left.shape != right.shape:\n",
        "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
        "    return torch.nn.Parameter(torch.tensor(right))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SbmSml7S41I"
      },
      "source": [
        "The actual updating of our model's inital random weights with OPEN AI's weights is done here where the load_weights_into_gpt function takes in our GPT model architecture and loads parameters from the OPEN AI to it. It also involves weight tying where the token embedding weights are themselves reused in the output layer thus reducing the overall parameter count from 162M to 124M."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "8I6pKKS3Rf9d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def load_weights_into_gpt(gpt, params):\n",
        "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
        "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
        "\n",
        "    for b in range(len(params[\"blocks\"])):\n",
        "        q_w, k_w, v_w = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
        "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
        "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
        "\n",
        "        q_b, k_b, v_b = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
        "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
        "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
        "\n",
        "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
        "            gpt.trf_blocks[b].att.out_proj.weight,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
        "            gpt.trf_blocks[b].att.out_proj.bias,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[0].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[0].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
        "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[2].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[2].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.trf_blocks[b].norm1.scale = assign(\n",
        "            gpt.trf_blocks[b].norm1.scale,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
        "        gpt.trf_blocks[b].norm1.shift = assign(\n",
        "            gpt.trf_blocks[b].norm1.shift,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
        "        gpt.trf_blocks[b].norm2.scale = assign(\n",
        "            gpt.trf_blocks[b].norm2.scale,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
        "        gpt.trf_blocks[b].norm2.shift = assign(\n",
        "            gpt.trf_blocks[b].norm2.shift,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
        "\n",
        "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
        "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
        "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlGG59oHUBTI"
      },
      "source": [
        "Demonstrating the model after loading the pre-trained weights from OPEN AI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "lBYq8VNSUJIq"
      },
      "outputs": [],
      "source": [
        "load_weights_into_gpt(gpt, params)\n",
        "gpt.to(device);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gcI4atpIUKJA",
        "outputId": "662499a7-e30f-4821-a04d-67e42c6354fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output text:\n",
            " Cristiano Ronaldo plays for the side he took three world record titles\n",
            "\n",
            "Cristiano Ronaldo signed a new professional-style transfer deal last week and\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(13)\n",
        "\n",
        "token_ids = generate(\n",
        "    model=gpt,\n",
        "    idx=text_to_token_ids(\"Cristiano Ronaldo plays for\", tokenizer).to(device),\n",
        "    max_new_tokens=25,\n",
        "    context_size=NEW_CONFIG[\"context_length\"],\n",
        "    top_k=50,\n",
        "    temperature=1.5\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdK_TDiNUYd5"
      },
      "source": [
        "Now we can see that the model is producing accurate and factual sentences since its been trained on millions of tokens across thousands of data sources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o0A56sTVKNH"
      },
      "source": [
        "# Finetuning our model for Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxYY-0kKVTHv"
      },
      "source": [
        "We can implement finetuning in 2 ways: Instruction based(requires user giving instructions or asking questions) and Classifcation based(no instructions are explicitly given and the model should just classify). We will implement the latter because they (Classification based fine-tuning) require computationally less memory and no explicit instructions and can be executed quickly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOB8K7RfWDON"
      },
      "source": [
        "We are trying to build an email classifier which classifies any email as either Spam or No Spam (also called as Ham). We can import the dataset containing both Spam and Ham emails along with their labels from the UC Irvine ML repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vxb5m6_cVRAw",
        "outputId": "2be78f06-5d6b-4145-a097-ccf2c53065d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded and saved as sms_spam_collection/SMSSpamCollection.tsv\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "import ssl\n",
        "import zipfile\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
        "zip_path = \"sms_spam_collection.zip\"\n",
        "extracted_path = \"sms_spam_collection\"\n",
        "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
        "\n",
        "def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):\n",
        "    if data_file_path.exists():\n",
        "        print(f\"{data_file_path} already exists. Skipping download and extraction.\")\n",
        "        return\n",
        "\n",
        "    # Create an unverified SSL context\n",
        "    ssl_context = ssl._create_unverified_context()\n",
        "\n",
        "    # Downloading the file\n",
        "    with urllib.request.urlopen(url, context=ssl_context) as response:\n",
        "        with open(zip_path, \"wb\") as out_file:\n",
        "            out_file.write(response.read())\n",
        "\n",
        "    # Unzipping the file\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(extracted_path)\n",
        "\n",
        "    # Add .tsv file extension\n",
        "    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
        "    os.rename(original_file_path, data_file_path)\n",
        "    print(f\"File downloaded and saved as {data_file_path}\")\n",
        "\n",
        "download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNnPKcJ5W4jj"
      },
      "source": [
        "Displaying a few rows to see if they are being loaded correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "9LRiBOg3W9_p",
        "outputId": "7cfe3cda-e5a0-41fa-bd1d-8604056d2066"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Label                                               Text\n",
              "0      ham  Go until jurong point, crazy.. Available only ...\n",
              "1      ham                      Ok lar... Joking wif u oni...\n",
              "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3      ham  U dun say so early hor... U c already then say...\n",
              "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
              "...    ...                                                ...\n",
              "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
              "5568   ham               Will ü b going to esplanade fr home?\n",
              "5569   ham  Pity, * was in mood for that. So...any other s...\n",
              "5570   ham  The guy did some bitching but I acted like i'd...\n",
              "5571   ham                         Rofl. Its true to its name\n",
              "\n",
              "[5572 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bb44a811-4ac6-4f35-946e-288d08651123\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5567</th>\n",
              "      <td>spam</td>\n",
              "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5568</th>\n",
              "      <td>ham</td>\n",
              "      <td>Will ü b going to esplanade fr home?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5569</th>\n",
              "      <td>ham</td>\n",
              "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5570</th>\n",
              "      <td>ham</td>\n",
              "      <td>The guy did some bitching but I acted like i'd...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5571</th>\n",
              "      <td>ham</td>\n",
              "      <td>Rofl. Its true to its name</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5572 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bb44a811-4ac6-4f35-946e-288d08651123')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bb44a811-4ac6-4f35-946e-288d08651123 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bb44a811-4ac6-4f35-946e-288d08651123');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f9e91280-fb50-4ac8-a37e-29ce87321fe6\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f9e91280-fb50-4ac8-a37e-29ce87321fe6')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f9e91280-fb50-4ac8-a37e-29ce87321fe6 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_58b256f9-6073-4ba4-80ef-e22feb499c92\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_58b256f9-6073-4ba4-80ef-e22feb499c92 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5572,\n  \"fields\": [\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"spam\",\n          \"ham\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5169,\n        \"samples\": [\n          \"K, makes sense, btw carlos is being difficult so you guys are gonna smoke while I go pick up the second batch and get gas\",\n          \"URGENT! Your mobile No *********** WON a \\u00a32,000 Bonus Caller Prize on 02/06/03! This is the 2nd attempt to reach YOU! Call 09066362220 ASAP! BOX97N7QP, 150ppm\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIASqjHcXCBL",
        "outputId": "229baaab-0e75-4446-bcaf-acebe06ca5b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label\n",
            "ham     4825\n",
            "spam     747\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(df[\"Label\"].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DowX3JxpXKd8"
      },
      "source": [
        "We can see that this dataset has more ham (4825) emails than spam (747) emails. For simplicity and the fact that we can fine-tune our LLM faster, we are going to make the count of both spam and ham, the same = 747"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQ1cSqhhXJzS",
        "outputId": "2e1bed86-816b-419a-f8b4-66920ff74e87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label\n",
            "ham     747\n",
            "spam    747\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "def create_balanced_dataset(df):\n",
        "\n",
        "    # Count the instances of \"spam\"\n",
        "    num_spam = df[df[\"Label\"] == \"spam\"].shape[0]\n",
        "\n",
        "    # Randomly sample \"ham\" instances to match the number of \"spam\" instances\n",
        "    ham_subset = df[df[\"Label\"] == \"ham\"].sample(num_spam, random_state=123)\n",
        "\n",
        "    # Combine ham \"subset\" with \"spam\"\n",
        "    balanced_df = pd.concat([ham_subset, df[df[\"Label\"] == \"spam\"]])\n",
        "\n",
        "    return balanced_df\n",
        "\n",
        "balanced_df = create_balanced_dataset(df)\n",
        "print(balanced_df[\"Label\"].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXpF-on5fsjF"
      },
      "source": [
        "We will also convert the string labels 'ham' to 0 and 'spam' to 1. This is similar to converting tokens to token_ids except here we only have 2 tokens (spam and ham)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "hN_iRywxfraS"
      },
      "outputs": [],
      "source": [
        "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHGZv04LgL6W"
      },
      "source": [
        "We will now split the whole balanced_df (747+747) into train_df(70%), validation_df(10%) and the remaining as test_df(20%)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "ariL_D71fr-W"
      },
      "outputs": [],
      "source": [
        "def random_split(df, train_frac, validation_frac):\n",
        "    # Shuffle the entire DataFrame\n",
        "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
        "\n",
        "    # Calculate split indices\n",
        "    train_end = int(len(df) * train_frac)\n",
        "    validation_end = train_end + int(len(df) * validation_frac)\n",
        "\n",
        "    # Split the DataFrame\n",
        "    train_df = df[:train_end]\n",
        "    validation_df = df[train_end:validation_end]\n",
        "    test_df = df[validation_end:]\n",
        "\n",
        "    return train_df, validation_df, test_df\n",
        "\n",
        "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
        "# Test size is implied to be 0.2 as the remainder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3Cm2I92gLYx",
        "outputId": "c077adb4-5e5b-4e1f-9524-03db5343e5e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1045\n",
            "149\n",
            "300\n"
          ]
        }
      ],
      "source": [
        "print(len(train_df))\n",
        "print(len(validation_df))\n",
        "print(len(test_df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDQl-H5JgrWp"
      },
      "source": [
        "We will save these 3 dataframes in csv format so that we can use them later dont have to do the split again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "FT-Z87pGgmLs"
      },
      "outputs": [],
      "source": [
        "train_df.to_csv(\"train.csv\", index=None)\n",
        "validation_df.to_csv(\"validation.csv\", index=None)\n",
        "test_df.to_csv(\"test.csv\", index=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIzY-sy6ozou"
      },
      "source": [
        "# Dataset() and Dataloaders() for fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HtDW1Gmow-9"
      },
      "source": [
        "We shall first tokenize the input text(same process as pre-training). Then we have to make sure all input_text have the same length. For this we shall look at the longest sequence of input text and pad up all the other tokens in other input_text with <|endoftext|> tokens. If not we can also truncate the rest of the tokens based on the smallest token_ids length. But this approach is not genrally done as informataion will be lost while truncating and this can be troublesome for the model to recognize spam and ham messages. From our dataset we get max_token_ids length to be 120. So we shall all pad other input tokens with the <|endoftext|> tokens such that they all have 120 token_ids which is basically number of columns. Also, we can consider 8 input_text per batch. So naturally the dimensions of the tensors will be (8 x 120). Since there are 1045 training input_text, total number of batches = 1045/8 ~ 130 batches. Now the final dimensions for the train_dataloader tensor will be (130 x 8 x 120). Similarly for validation tensor, its 149/8 ~ 19. So the dimensions = (19 x 8 120). For the testing tensor, 300/8 ~ = 38. Dim = (39 x 8 x 120)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuQIb0FsxJYg"
      },
      "source": [
        "We first need to implement a PyTorch Dataset, which specifies how the data is loaded and processed, before we can instantiate the data loaders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "WN13fph6gpwz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class SpamDataset(Dataset):\n",
        "    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "\n",
        "        # Pre-tokenize texts\n",
        "        self.encoded_texts = [\n",
        "            tokenizer.encode(text) for text in self.data[\"Text\"]\n",
        "        ]\n",
        "\n",
        "        if max_length is None:\n",
        "            self.max_length = self._longest_encoded_length()\n",
        "        else:\n",
        "            self.max_length = max_length\n",
        "\n",
        "            # Truncate sequences if they are longer than max_length\n",
        "            self.encoded_texts = [\n",
        "                encoded_text[:self.max_length]\n",
        "                for encoded_text in self.encoded_texts\n",
        "            ]\n",
        "\n",
        "        # Pad sequences to the longest sequence\n",
        "        self.encoded_texts = [\n",
        "            encoded_text + [pad_token_id] * (self.max_length - len(encoded_text))\n",
        "            for encoded_text in self.encoded_texts\n",
        "        ]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        encoded = self.encoded_texts[index]\n",
        "        label = self.data.iloc[index][\"Label\"]\n",
        "        return (\n",
        "            torch.tensor(encoded, dtype=torch.long),\n",
        "            torch.tensor(label, dtype=torch.long)\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def _longest_encoded_length(self):\n",
        "        max_length = 0\n",
        "        for encoded_text in self.encoded_texts:\n",
        "            encoded_length = len(encoded_text)\n",
        "            if encoded_length > max_length:\n",
        "                max_length = encoded_length\n",
        "        return max_length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LyuyaWnxUBd"
      },
      "source": [
        "Checking the max_length for train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shZmDLl1xUmV",
        "outputId": "d4dffd56-6c3f-419f-acf1-72e558bb0671"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "120\n"
          ]
        }
      ],
      "source": [
        "train_dataset = SpamDataset(\n",
        "    csv_file=\"train.csv\",\n",
        "    max_length=None,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "print(train_dataset.max_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Xt_G8PTx09G"
      },
      "source": [
        "Similarly for validation and testing dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RA68btsixjHg",
        "outputId": "29944515-fb54-4842-8869-d76392c5ce22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "120\n"
          ]
        }
      ],
      "source": [
        "val_dataset = SpamDataset(\n",
        "    csv_file=\"validation.csv\",\n",
        "    max_length=train_dataset.max_length,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "test_dataset = SpamDataset(\n",
        "    csv_file=\"test.csv\",\n",
        "    max_length=train_dataset.max_length,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "print(test_dataset.max_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0BnMVz1x_wM"
      },
      "source": [
        "Implementing dataloaders for the 3 types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "SZTTXzNmxnYW"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "num_workers = 0\n",
        "batch_size = 8\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=True,\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    dataset=val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=False,\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVwUR0RuyJOK"
      },
      "source": [
        "Verifying the dimensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMtAabR_yGfV",
        "outputId": "d1ab462c-793c-40f2-afab-636b8ec9a3c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loader:\n",
            "Input batch dimensions: torch.Size([2, 256])\n",
            "Label batch dimensions torch.Size([2, 256])\n"
          ]
        }
      ],
      "source": [
        "print(\"Train loader:\")\n",
        "for input_batch, target_batch in train_loader:\n",
        "    pass\n",
        "\n",
        "print(\"Input batch dimensions:\", input_batch.shape)\n",
        "print(\"Label batch dimensions\", target_batch.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcGwiy84yMsy"
      },
      "source": [
        "The label tensor stores the class labels(spam/ham -> 0/1) corresponding to the 8 training examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqHrVm_iyxfF"
      },
      "source": [
        "Verifying the batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HazHvjxMyWYk",
        "outputId": "19c948a8-127f-4206-bc49-8c953b9ff646"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "130 training batches\n",
            "19 validation batches\n",
            "38 test batches\n"
          ]
        }
      ],
      "source": [
        "print(f\"{len(train_loader)} training batches\")\n",
        "print(f\"{len(val_loader)} validation batches\")\n",
        "print(f\"{len(test_loader)} test batches\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hl8UhP3By85L"
      },
      "source": [
        "This whole above process is the data pre-processing before finetuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tSQ2E8hzFkX"
      },
      "source": [
        "# Initializing a model with pre-trained weights that we had downloaded from Open AI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "MDH_2hxdy2O7"
      },
      "outputs": [],
      "source": [
        "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
        "INPUT_PROMPT = \"Every effort moves\"\n",
        "\n",
        "BASE_CONFIG = {\n",
        "    \"vocab_size\": 50257,     # Vocabulary size\n",
        "    \"context_length\": 1024,  # Context length\n",
        "    \"drop_rate\": 0.0,        # Dropout rate\n",
        "    \"qkv_bias\": True         # Query-key-value bias\n",
        "}\n",
        "\n",
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}\n",
        "\n",
        "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
        "\n",
        "assert train_dataset.max_length <= BASE_CONFIG[\"context_length\"], (\n",
        "    f\"Dataset length {train_dataset.max_length} exceeds model's context \"\n",
        "    f\"length {BASE_CONFIG['context_length']}. Reinitialize data sets with \"\n",
        "    f\"`max_length={BASE_CONFIG['context_length']}`\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bZ-uqwpzhbP"
      },
      "source": [
        "Loading the downloaded weights into the GPT model just like how we did for pre-training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6pAIuZUzYZ2",
        "outputId": "54ebdff0-fa36-4036-9ea9-94447e3df63a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists and is up-to-date: gpt2/124M/checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists and is up-to-date: gpt2/124M/encoder.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists and is up-to-date: gpt2/124M/hparams.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"
          ]
        }
      ],
      "source": [
        "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
        "\n",
        "from gpt_download3 import download_and_load_gpt2\n",
        "\n",
        "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
        "\n",
        "model = GPTModel(BASE_CONFIG)\n",
        "load_weights_into_gpt(model, params)\n",
        "model.eval();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEPYiKu6zZkB",
        "outputId": "e967f199-9c8d-408f-a5e4-41789f5b2ad4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cristiano Ronaldo is the best football player in the world.\n",
            "\n",
            "The Portuguese has been a key figure in the club's success since he joined from Real Madrid in 2012\n",
            "Is the following text 'spam'? Answer with 'yes' or 'no': 'You are a winner you have been specially selected to receive $1000 cash or a $2000 award.'\n",
            "\n",
            "The following text 'spam'? Answer with 'yes' or 'no': 'You are a winner\n"
          ]
        }
      ],
      "source": [
        "text_1 = \"Cristiano Ronaldo is the best football player in\"\n",
        "\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(text_1, tokenizer),\n",
        "    max_new_tokens=25,\n",
        "    context_size=BASE_CONFIG[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(token_ids_to_text(token_ids, tokenizer))\n",
        "\n",
        "\n",
        "text_2 = (\n",
        "    \"Is the following text 'spam'? Answer with 'yes' or 'no':\"\n",
        "    \" 'You are a winner you have been specially\"\n",
        "    \" selected to receive $1000 cash or a $2000 award.'\"\n",
        ")\n",
        "\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(text_2, tokenizer),\n",
        "    max_new_tokens=23,\n",
        "    context_size=BASE_CONFIG[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvKGZnXn2yVw"
      },
      "source": [
        "As we can see, the model is able to generate new text but NOT able to classify the text even upon giving explicit instructions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdSqGR3U_7Gb"
      },
      "source": [
        "# Adding a Classification Head to the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30z08j_DAGqm"
      },
      "source": [
        "We replace the original output layer, which maps the hidden representation to a vocabulary of 50,257, with a smaller output layer that maps to two classes: 0 (\"not spam\") and 1 (\"spam\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETEogP6bAPX1"
      },
      "source": [
        "We could technically use a single output node since we are dealing with a binary classification task.\n",
        "\n",
        "However, this would require modifying the loss function.\n",
        "\n",
        "Therefore, we choose a more general approach where the number of output nodes matches the number of classes.\n",
        "\n",
        "For example, for a 3-class problem, such as classifying news articles as \"Technology\", \"Sports\", or \"Politics\", we would use three output nodes, and so forth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7ki3nNhm2Zhh",
        "outputId": "a5203674-b31f-4f6b-b87a-e04bb7030557"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPTModel(\n",
            "  (tok_emb): Embedding(50257, 768)\n",
            "  (pos_emb): Embedding(1024, 768)\n",
            "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
            "  (trf_blocks): Sequential(\n",
            "    (0): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (1): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (2): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (3): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (4): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (5): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (6): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (7): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (8): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (9): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (10): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (11): TransformerBlock(\n",
            "      (att): MultiHeadAttention(\n",
            "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ff): FeedForward(\n",
            "        (layers): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (1): GELU()\n",
            "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norm1): LayerNorm()\n",
            "      (norm2): LayerNorm()\n",
            "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (final_norm): LayerNorm()\n",
            "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdAjOEO-DLXJ"
      },
      "source": [
        "We shall first freeze all the model parameters which will make them non-trainable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "RWpj6xzW_ypw"
      },
      "outputs": [],
      "source": [
        "for param in model.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThUd_syjENkD"
      },
      "source": [
        "To implement the classification fine tuning:\n",
        "1) Final output layer should have only 2 classes.\n",
        "\n",
        "2) The 12th block of the transformer will be the only trainable block to make the training time short.\n",
        "\n",
        "3) Final normalization layer parameters will also be set to true."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8opH9CqDZbj"
      },
      "source": [
        "We shall only have 2 classes in the output layer since we only want our model to classify whether the input text is 'spam' or 'ham'. Also the new out_head will have its 'requires_grad' attribute set to 'True' by default. So currently that is the only trainable layer by default."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "yOHkrbrnAsS5"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "num_classes = 2\n",
        "model.out_head = torch.nn.Linear(in_features=BASE_CONFIG[\"emb_dim\"], out_features=num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GT1mk0AlE6MX"
      },
      "source": [
        "Similarly we shall do set the requires_grad = True for 12th block of the Transformer as well as the final norm layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "hrCyB3tzAung"
      },
      "outputs": [],
      "source": [
        "for param in model.trf_blocks[-1].parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "for param in model.final_norm.parameters():\n",
        "    param.requires_grad = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7jBLD41FWZM"
      },
      "source": [
        "Checking for an example text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtTgiF9cAwvf",
        "outputId": "5b70396f-6581-4a7d-ad7a-a79cc409cf02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs: tensor([[5211,  345,  423,  640]])\n",
            "Inputs dimensions: torch.Size([1, 4])\n"
          ]
        }
      ],
      "source": [
        "inputs = tokenizer.encode(\"Do you have time\")\n",
        "inputs = torch.tensor(inputs).unsqueeze(0)\n",
        "print(\"Inputs:\", inputs)\n",
        "print(\"Inputs dimensions:\", inputs.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRliUJcVBI6H",
        "outputId": "aac0b339-1afe-41e0-ecd9-3f58e41a8403"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outputs:\n",
            " tensor([[[-1.5854,  0.9904],\n",
            "         [-3.7235,  7.4548],\n",
            "         [-2.2661,  6.6049],\n",
            "         [-3.5983,  3.9902]]])\n",
            "Outputs dimensions: torch.Size([1, 4, 2])\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    outputs = model(inputs)\n",
        "\n",
        "print(\"Outputs:\\n\", outputs)\n",
        "print(\"Outputs dimensions:\", outputs.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WjdwMd2BLRZ",
        "outputId": "b575e0d1-ce2d-484a-eb50-252aa0938980"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last output token: tensor([[-3.5983,  3.9902]])\n"
          ]
        }
      ],
      "source": [
        "print(\"Last output token:\", outputs[:, -1, :])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CA-ufQmNBOps",
        "outputId": "6b1df012-7c19-4ce0-f667-29421a4dbce5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class label: 1\n"
          ]
        }
      ],
      "source": [
        "probas = torch.softmax(outputs[:, -1, :], dim=-1)\n",
        "label = torch.argmax(probas)\n",
        "print(\"Class label:\", label.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwB1h-IYFgn1"
      },
      "source": [
        "Here, instead of softmax, we can use argmax since we only want the classification, so essentially we want the model to reutrn its highest value more clearly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EX-ofxQ1F4Yu"
      },
      "source": [
        "Function implementing the argmax-based prediction code to all examples in the dataset and calculate the proportion of correct predictions (Returns a ratio)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "CGnEEvZwBRcJ"
      },
      "outputs": [],
      "source": [
        "def calc_accuracy_loader(data_loader, model, device, num_batches=None):\n",
        "    model.eval()\n",
        "    correct_predictions, num_examples = 0, 0\n",
        "\n",
        "    if num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "      #If num_batches are given\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                logits = model(input_batch)[:, -1, :]  # Logits of last output token\n",
        "            predicted_labels = torch.argmax(logits, dim=-1)\n",
        "\n",
        "            num_examples += predicted_labels.shape[0]\n",
        "            correct_predictions += (predicted_labels == target_batch).sum().item()\n",
        "        else:\n",
        "            break\n",
        "    return correct_predictions / num_examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kT3PVp_9Gf3N"
      },
      "source": [
        "Using the function above to run 10 batches(out of the 130) and print the accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "tyGOLWa1BhkF",
        "outputId": "5271a3ef-1f6f-47dc-d7af-4f7c72954a4e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (2) must match the size of tensor b (256) at non-singleton dimension 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-91-78892b43d319>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m123\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# For reproducibility due to the shuffling in the training data loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mtrain_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_accuracy_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mval_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_accuracy_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_accuracy_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-90-ed0bac7ceaff>\u001b[0m in \u001b[0;36mcalc_accuracy_loader\u001b[0;34m(data_loader, model, device, num_batches)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mnum_examples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpredicted_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mcorrect_predictions\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted_labels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtarget_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (256) at non-singleton dimension 1"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Note:\n",
        "# Uncommenting the following lines will allow the code to run on Apple Silicon chips, if applicable,\n",
        "# which is approximately 2x faster than on an Apple CPU (as measured on an M3 MacBook Air).\n",
        "# As of this writing, in PyTorch 2.4, the results obtained via CPU and MPS were identical.\n",
        "# However, in earlier versions of PyTorch, you may observe different results when using MPS.\n",
        "\n",
        "#if torch.cuda.is_available():\n",
        "#    device = torch.device(\"cuda\")\n",
        "#elif torch.backends.mps.is_available():\n",
        "#    device = torch.device(\"mps\")\n",
        "#else:\n",
        "#    device = torch.device(\"cpu\")\n",
        "#print(f\"Running on {device} device.\")\n",
        "\n",
        "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
        "\n",
        "torch.manual_seed(123) # For reproducibility due to the shuffling in the training data loader\n",
        "\n",
        "train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=10)\n",
        "val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=10)\n",
        "test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=10)\n",
        "\n",
        "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
        "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
        "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb1EGKFaH0pI"
      },
      "source": [
        "As seen above, the prediction accuracy is not even 50% which is worse than a coin toss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhDYHWCUIljR"
      },
      "source": [
        "Classification accuracy is not a differentiable function, so we use cross entropy loss as a proxy to maximize accuracy.\n",
        "\n",
        "Cross Entropy loss = for i from 1 to n, -∑ yi log pi where 'yi' is the actual correct value and 'pi' is the predicted value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "M-_SfNV2BliA"
      },
      "outputs": [],
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)[:, -1, :]  # Logits of last output token\n",
        "    loss = torch.nn.functional.cross_entropy(logits, target_batch)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7UQr8Z7L3ty"
      },
      "source": [
        "Implementing a function which calculates cross entropy loss for dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dXwsVYbgBo1j"
      },
      "outputs": [],
      "source": [
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        # Reduce the number of batches to match the total number of batches in the data loader\n",
        "        # if num_batches exceeds the number of batches in the data loader\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IQ2qQY4BBuHh",
        "outputId": "449ded10-c8ce-4510-d3ce-26502fe88118"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 2.453\n",
            "Validation loss: 2.583\n",
            "Test loss: 2.322\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
        "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
        "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
        "    test_loss = calc_loss_loader(test_loader, model, device, num_batches=5)\n",
        "\n",
        "print(f\"Training loss: {train_loss:.3f}\")\n",
        "print(f\"Validation loss: {val_loss:.3f}\")\n",
        "print(f\"Test loss: {test_loss:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhPGWTQfMqTn"
      },
      "source": [
        "As we can tell, all loss values are high."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCo07e0GMycK"
      },
      "source": [
        "Implementing Training loop which includes backpropagation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-xOXuNtqBwFW"
      },
      "outputs": [],
      "source": [
        "def train_classifier_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                            eval_freq, eval_iter):\n",
        "    # Initialize lists to track losses and examples seen\n",
        "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
        "    examples_seen, global_step = 0, -1\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward() # Calculate loss gradients\n",
        "            optimizer.step() # Update model weights using loss gradients\n",
        "            examples_seen += input_batch.shape[0] # New: track examples instead of tokens\n",
        "            global_step += 1\n",
        "\n",
        "            # Optional evaluation step\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # Calculate accuracy after each epoch\n",
        "        train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "        print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n",
        "        print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
        "        train_accs.append(train_accuracy)\n",
        "        val_accs.append(val_accuracy)\n",
        "\n",
        "    return train_losses, val_losses, train_accs, val_accs, examples_seen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AALZZwTNM-18"
      },
      "source": [
        "Optional function which returns train_loss and val_loss after every 50 steps (eval_freq)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Mm9ghw5tB3xv"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QrmxywEpB6wN",
        "outputId": "aa79c27d-75c3-49a3-8797-35100ab5cd47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ep 1 (Step 000000): Train loss 2.153, Val loss 2.392\n",
            "Ep 1 (Step 000050): Train loss 0.617, Val loss 0.637\n",
            "Ep 1 (Step 000100): Train loss 0.523, Val loss 0.557\n",
            "Training accuracy: 70.00% | Validation accuracy: 72.50%\n",
            "Ep 2 (Step 000150): Train loss 0.561, Val loss 0.489\n",
            "Ep 2 (Step 000200): Train loss 0.419, Val loss 0.397\n",
            "Ep 2 (Step 000250): Train loss 0.409, Val loss 0.353\n",
            "Training accuracy: 82.50% | Validation accuracy: 85.00%\n",
            "Ep 3 (Step 000300): Train loss 0.333, Val loss 0.320\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
        "\n",
        "num_epochs = 5\n",
        "train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=50, eval_iter=5,\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLBhpH6HNk0V"
      },
      "source": [
        "Plotting Training vs Validation Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvZH-2RiB9lA"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_values(epochs_seen, examples_seen, train_values, val_values, label=\"loss\"):\n",
        "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "\n",
        "    # Plot training and validation loss against epochs\n",
        "    ax1.plot(epochs_seen, train_values, label=f\"Training {label}\")\n",
        "    ax1.plot(epochs_seen, val_values, linestyle=\"-.\", label=f\"Validation {label}\")\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(label.capitalize())\n",
        "    ax1.legend()\n",
        "\n",
        "    # Create a second x-axis for examples seen\n",
        "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
        "    ax2.plot(examples_seen, train_values, alpha=0)  # Invisible plot for aligning ticks\n",
        "    ax2.set_xlabel(\"Examples seen\")\n",
        "\n",
        "    fig.tight_layout()  # Adjust layout to make room\n",
        "    plt.savefig(f\"{label}-plot.pdf\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8-BIWweCE_K"
      },
      "outputs": [],
      "source": [
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
        "\n",
        "plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRvA-264Nr93"
      },
      "source": [
        "Plotting Training vs Validation Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFxe7bDxCFs_"
      },
      "outputs": [],
      "source": [
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_accs))\n",
        "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_accs))\n",
        "\n",
        "plot_values(epochs_tensor, examples_seen_tensor, train_accs, val_accs, label=\"accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4QE1cFfN1w6"
      },
      "source": [
        "Accuracies of Training, Validation and Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mgSdqi9CJHu"
      },
      "outputs": [],
      "source": [
        "train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
        "val_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
        "test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
        "\n",
        "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
        "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
        "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGXmlXZnOOUs"
      },
      "source": [
        "A holistic function which takes in the input text, convert them to tokens, then to token_ids, pad according to the longest sequence, pass it to the model which returns output logits and classify based on the argmax() function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyD7y1ZwCLak"
      },
      "outputs": [],
      "source": [
        "def classify_review(text, model, tokenizer, device, max_length=None, pad_token_id=50256):\n",
        "    model.eval()\n",
        "\n",
        "    # Prepare inputs to the model\n",
        "    input_ids = tokenizer.encode(text)\n",
        "    supported_context_length = model.pos_emb.weight.shape[0]\n",
        "\n",
        "    # Truncate sequences if they too long\n",
        "    input_ids = input_ids[:min(max_length, supported_context_length)]\n",
        "\n",
        "    # Pad sequences to the longest sequence\n",
        "    input_ids += [pad_token_id] * (max_length - len(input_ids))\n",
        "    input_tensor = torch.tensor(input_ids, device=device).unsqueeze(0) # add batch dimension\n",
        "\n",
        "    # Model inference\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_tensor)[:, -1, :]  # Logits of the last output token\n",
        "    predicted_label = torch.argmax(logits, dim=-1).item()\n",
        "\n",
        "    # Return the classified result\n",
        "    return \"spam\" if predicted_label == 1 else \"not spam\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgjt8pGRPgpy"
      },
      "source": [
        "Using the function over example 'spam' and 'ham' input text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8B2zfzj7CQyI"
      },
      "outputs": [],
      "source": [
        "text_1 = (\n",
        "    \"You are a winner you have been specially\"\n",
        "    \" selected to receive $1000 cash or a $2000 award.\"\n",
        ")\n",
        "\n",
        "print(classify_review(\n",
        "    text_1, model, tokenizer, device, max_length=train_dataset.max_length\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57L6m8cbCTWX"
      },
      "outputs": [],
      "source": [
        "text_2 = (\n",
        "    \"Hey, just wanted to check if we're still on\"\n",
        "    \" for dinner tonight? Let me know!\"\n",
        ")\n",
        "\n",
        "print(classify_review(\n",
        "    text_2, model, tokenizer, device, max_length=train_dataset.max_length\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnJ5KBI8Pp5F"
      },
      "source": [
        "As we can see, the model can now correctly classify the input text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1Fna0Scjnmb"
      },
      "source": [
        "Saving the model and loading it back."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3InsmwXnCVbD"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"review_classifier.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B32GLcavjcr4"
      },
      "outputs": [],
      "source": [
        "model_state_dict = torch.load(\"review_classifier.pth\")\n",
        "model.load_state_dict(model_state_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiHmGFfDjtVy"
      },
      "source": [
        "# Instruction based finetuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SF03osKj8n1"
      },
      "source": [
        "In this section, we download and format the instruction dataset for instruction finetuning a pretrained LLM in this chapter. The dataset consists of 1100 instruction-response pairs.\n",
        "\n",
        "The following code implements and executes a function to download this dataset, which is a relatively small file, only 204 KB in size, in JSON format. JSON, or JavaScript Object Notation, mirrors the structure of Python dictionaries, providing a simple structure for data interchange that is both human-readable and machine-friendly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "zqgaXb3mjvuh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90fac49c-f801-4cff-d6d5-54d100e10ee5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of entries: 1100\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "import urllib\n",
        "import ssl\n",
        "\n",
        "def download_and_load_file(file_path, url):\n",
        "    ssl_context = ssl.create_default_context()\n",
        "    ssl_context.check_hostname = False\n",
        "    ssl_context.verify_mode = ssl.CERT_NONE\n",
        "\n",
        "    if not os.path.exists(file_path):\n",
        "        with urllib.request.urlopen(url, context=ssl_context) as response:\n",
        "            text_data = response.read().decode(\"utf-8\")\n",
        "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "            file.write(text_data)\n",
        "    else:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "            text_data = file.read()\n",
        "\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "file_path = \"instruction-data.json\"\n",
        "url = (\n",
        "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n",
        "    \"/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
        ")\n",
        "\n",
        "data = download_and_load_file(file_path, url)\n",
        "print(\"Number of entries:\", len(data))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "AgWBorC6kYzw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76d4dc60-c782-41c3-d6e4-0cef86367d70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example entry:\n",
            " {'instruction': 'Identify the correct spelling of the following word.', 'input': 'Ocassion', 'output': \"The correct spelling is 'Occasion.'\"}\n"
          ]
        }
      ],
      "source": [
        "print(\"Example entry:\\n\", data[50])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "Kt5Kv0MzkZar",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e40e767c-cbb1-4b03-f437-6bc9857dc80b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Another example entry:\n",
            " {'instruction': \"What is an antonym of 'complicated'?\", 'input': '', 'output': \"An antonym of 'complicated' is 'simple'.\"}\n"
          ]
        }
      ],
      "source": [
        "print(\"Another example entry:\\n\", data[999])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo2mcA7ukgev"
      },
      "source": [
        "Converting Instructions into Alpaca format by Stanford"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRoJUkd3knzK"
      },
      "source": [
        "Note: Alpaca prompt style template uses 'Instruction', 'Input' and 'Response' format. We can also use the Phi3 by Microsoft which uses 'User' and 'Assistant' format and here the 'Insruction' and 'Input' are embedded within the 'User' prompt and the 'Response' is embedded within the 'Assistant' prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvKNmZvll5Qo"
      },
      "source": [
        "This format_input function takes a dictionary entry as input and constructs a formatted string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "JWxvkr80kbY9"
      },
      "outputs": [],
      "source": [
        "def format_input(entry):\n",
        "    instruction_text = (\n",
        "        f\"Below is an instruction that describes a task. \"\n",
        "        f\"Write a response that appropriately completes the request.\"\n",
        "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
        "    )\n",
        "\n",
        "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
        "\n",
        "    return instruction_text + input_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "ahhHkF9zlsef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46647f4b-50e4-4e3a-9144-273839ba2a47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Identify the correct spelling of the following word.\n",
            "\n",
            "### Input:\n",
            "Ocassion\n",
            "\n",
            "### Response:\n",
            "The correct spelling is 'Occasion.'\n"
          ]
        }
      ],
      "source": [
        "model_input = format_input(data[50])\n",
        "desired_response = f\"\\n\\n### Response:\\n{data[50]['output']}\"\n",
        "\n",
        "print(model_input + desired_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCmn6gYdmAz8"
      },
      "source": [
        "Note that the format_input skips the optional ### Input: section if the 'input' field is empty, which we can test out by applying the format_input function to entry data[999] that we inspected earlier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "Fr_Ry7Rqkfo0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecb971af-48c9-4397-f641-fe7654f2252d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is an antonym of 'complicated'?\n",
            "\n",
            "### Response:\n",
            "An antonym of 'complicated' is 'simple'.\n"
          ]
        }
      ],
      "source": [
        "model_input = format_input(data[999])\n",
        "desired_response = f\"\\n\\n### Response:\\n{data[999]['output']}\"\n",
        "\n",
        "print(model_input + desired_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktEXjiGymIkz"
      },
      "source": [
        "Splitting the dataset for Train-Test-Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "Wxg--_Q2mDgl"
      },
      "outputs": [],
      "source": [
        "train_portion = int(len(data) * 0.85)  # 85% for training\n",
        "test_portion = int(len(data) * 0.1)    # 10% for testing\n",
        "val_portion = len(data) - train_portion - test_portion  # Remaining 5% for validation\n",
        "\n",
        "train_data = data[:train_portion]\n",
        "test_data = data[train_portion:train_portion + test_portion]\n",
        "val_data = data[train_portion + test_portion:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "wbmstspCmGeE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e837b01a-5ed2-4f9d-ff8d-74341da2528a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set length: 935\n",
            "Validation set length: 55\n",
            "Test set length: 110\n"
          ]
        }
      ],
      "source": [
        "print(\"Training set length:\", len(train_data))\n",
        "print(\"Validation set length:\", len(val_data))\n",
        "print(\"Test set length:\", len(test_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrWZL8VyrkVp"
      },
      "source": [
        "Function which can take in the inputs and tokenize it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "pbHvHQ9JmOCt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class InstructionDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.data = data\n",
        "\n",
        "        # Pre-tokenize texts\n",
        "        self.encoded_texts = []\n",
        "        for entry in data:\n",
        "            instruction_plus_input = format_input(entry)\n",
        "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
        "            full_text = instruction_plus_input + response_text\n",
        "            self.encoded_texts.append(\n",
        "                tokenizer.encode(full_text)\n",
        "            )\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.encoded_texts[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "NAvGpNC3mZIB"
      },
      "outputs": [],
      "source": [
        "def custom_collate_fn(\n",
        "    batch,\n",
        "    pad_token_id=50256,\n",
        "    ignore_index=-100,\n",
        "    allowed_max_length=None,\n",
        "    device=\"cpu\"\n",
        "):\n",
        "    # Find the longest sequence in the batch\n",
        "    batch_max_length = max(len(item)+1 for item in batch)\n",
        "\n",
        "    # Pad and prepare inputs and targets\n",
        "    inputs_lst, targets_lst = [], []\n",
        "\n",
        "    for item in batch:\n",
        "        new_item = item.copy()\n",
        "        # Add an <|endoftext|> token\n",
        "        new_item += [pad_token_id]\n",
        "        # Pad sequences to max_length\n",
        "        padded = (\n",
        "            new_item + [pad_token_id] *\n",
        "            (batch_max_length - len(new_item))\n",
        "        )\n",
        "        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n",
        "        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
        "\n",
        "        # New: Replace all but the first padding tokens in targets by ignore_index\n",
        "        mask = targets == pad_token_id\n",
        "        indices = torch.nonzero(mask).squeeze()\n",
        "        if indices.numel() > 1:\n",
        "            targets[indices[1:]] = ignore_index\n",
        "\n",
        "        # New: Optionally truncate to maximum sequence length\n",
        "        if allowed_max_length is not None:\n",
        "            inputs = inputs[:allowed_max_length]\n",
        "            targets = targets[:allowed_max_length]\n",
        "\n",
        "        inputs_lst.append(inputs)\n",
        "        targets_lst.append(targets)\n",
        "\n",
        "    # Convert list of inputs and targets to tensors and transfer to target device\n",
        "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
        "    targets_tensor = torch.stack(targets_lst).to(device)\n",
        "\n",
        "    return inputs_tensor, targets_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "Ve9i6IsPnFDA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f2a2237-8a68-42e0-c67c-1fc4da21e0d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    0,     1,     2,     3,     4],\n",
            "        [    5,     6, 50256, 50256, 50256],\n",
            "        [    7,     8,     9, 50256, 50256]])\n",
            "tensor([[    1,     2,     3,     4, 50256],\n",
            "        [    6, 50256,  -100,  -100,  -100],\n",
            "        [    8,     9, 50256,  -100,  -100]])\n"
          ]
        }
      ],
      "source": [
        "inputs_1 = [0, 1, 2, 3, 4]\n",
        "inputs_2 = [5, 6]\n",
        "inputs_3 = [7, 8, 9]\n",
        "\n",
        "batch = (\n",
        "    inputs_1,\n",
        "    inputs_2,\n",
        "    inputs_3\n",
        ")\n",
        "\n",
        "inputs, targets = custom_collate_fn(batch)\n",
        "print(inputs)\n",
        "print(targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "xEk-iV5ts_44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8e817ee-01dd-4535-a7cc-e1634765b0c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.1269)\n",
            "tensor(0.7936)\n",
            "tensor(1.1269)\n",
            "loss_1 == loss_3: tensor(True)\n"
          ]
        }
      ],
      "source": [
        "logits_1 = torch.tensor(\n",
        "    [[-1.0, 1.0],  # 1st training example\n",
        "     [-0.5, 1.5]]  # 2nd training example\n",
        ")\n",
        "targets_1 = torch.tensor([0, 1])\n",
        "\n",
        "\n",
        "loss_1 = torch.nn.functional.cross_entropy(logits_1, targets_1)\n",
        "print(loss_1)\n",
        "\n",
        "\n",
        "\n",
        "logits_2 = torch.tensor(\n",
        "    [[-1.0, 1.0],\n",
        "     [-0.5, 1.5],\n",
        "     [-0.5, 1.5]]  # New 3rd training example\n",
        ")\n",
        "targets_2 = torch.tensor([0, 1, 1])\n",
        "\n",
        "loss_2 = torch.nn.functional.cross_entropy(logits_2, targets_2)\n",
        "print(loss_2)\n",
        "\n",
        "\n",
        "\n",
        "logits_2 = torch.tensor(\n",
        "    [[-1.0, 1.0],\n",
        "     [-0.5, 1.5],\n",
        "     [-0.5, 1.5]]  # New 3rd training example\n",
        ")\n",
        "\n",
        "targets_3 = torch.tensor([0, 1, -100])\n",
        "\n",
        "loss_3 = torch.nn.functional.cross_entropy(logits_2, targets_3)\n",
        "print(loss_3)\n",
        "print(\"loss_1 == loss_3:\", loss_1 == loss_3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nluG51jtkPg"
      },
      "source": [
        "Based on this result, we can see that the resulting loss on these 3 training examples is identical to the loss we calculated from the 2 training examples earlier.\n",
        "\n",
        "In other words, the cross entropy loss function ignored the third entry in the targets_3 vector, the token ID corresponding to -100.\n",
        "\n",
        "(Interested readers can try to replace the -100 value with another token IDs that is not 0 or 1, and will see that this results in an error.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "2g6WEvBCs_2k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7c4dacf-b850-4f02-c679-fa9f0bb21712"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Note:\n",
        "# Uncommenting the following lines will allow the code to run on Apple Silicon chips, if applicable,\n",
        "# which is much faster than on an Apple CPU (as measured on an M3 MacBook Air).\n",
        "# However, the resulting loss values may be slightly different.\n",
        "\n",
        "#if torch.cuda.is_available():\n",
        "#    device = torch.device(\"cuda\")\n",
        "#elif torch.backends.mps.is_available():\n",
        "#    device = torch.device(\"mps\")\n",
        "#else:\n",
        "#    device = torch.device(\"cpu\")\n",
        "\n",
        "print(\"Device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "i_SDSoqqs_sf"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "customized_collate_fn = partial(custom_collate_fn, device=device, allowed_max_length=1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "Zn0dXYcGs_ht"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "num_workers = 0\n",
        "batch_size = 8\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_dataset = InstructionDataset(train_data, tokenizer)\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=customized_collate_fn,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "val_dataset = InstructionDataset(val_data, tokenizer)\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=customized_collate_fn,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "test_dataset = InstructionDataset(test_data, tokenizer)\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=customized_collate_fn,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=num_workers\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "_7G0Vk5Hs_Lw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ca7a48f-acb6-4dca-d31b-bde2fbf33145"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loader:\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 76]) torch.Size([8, 76])\n",
            "torch.Size([8, 73]) torch.Size([8, 73])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 62]) torch.Size([8, 62])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 62]) torch.Size([8, 62])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 77]) torch.Size([8, 77])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 79]) torch.Size([8, 79])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 60]) torch.Size([8, 60])\n",
            "torch.Size([8, 59]) torch.Size([8, 59])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 76]) torch.Size([8, 76])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 91]) torch.Size([8, 91])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 89]) torch.Size([8, 89])\n",
            "torch.Size([8, 59]) torch.Size([8, 59])\n",
            "torch.Size([8, 88]) torch.Size([8, 88])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 76]) torch.Size([8, 76])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 60]) torch.Size([8, 60])\n",
            "torch.Size([8, 60]) torch.Size([8, 60])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 58]) torch.Size([8, 58])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 87]) torch.Size([8, 87])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 60]) torch.Size([8, 60])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 57]) torch.Size([8, 57])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 62]) torch.Size([8, 62])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 91]) torch.Size([8, 91])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 81]) torch.Size([8, 81])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 82]) torch.Size([8, 82])\n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 77]) torch.Size([8, 77])\n",
            "torch.Size([8, 91]) torch.Size([8, 91])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 78]) torch.Size([8, 78])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n"
          ]
        }
      ],
      "source": [
        "print(\"Train loader:\")\n",
        "for inputs, targets in train_loader:\n",
        "    print(inputs.shape, targets.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "tyrQ0WFft3u7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc725486-1415-4adf-c697-8493acd29298"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 171kiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "encoder.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 2.69MiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "hparams.json: 100%|██████████| 91.0/91.0 [00:00<00:00, 222kiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "model.ckpt.data-00000-of-00001: 100%|██████████| 3.10G/3.10G [04:14<00:00, 12.2MiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "model.ckpt.index: 100%|██████████| 15.5k/15.5k [00:00<00:00, 21.5MiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "model.ckpt.meta: 100%|██████████| 1.38M/1.38M [00:00<00:00, 3.21MiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "vocab.bpe: 100%|██████████| 456k/456k [00:00<00:00, 1.39MiB/s]\n"
          ]
        }
      ],
      "source": [
        "from gpt_download3 import download_and_load_gpt2\n",
        "\n",
        "BASE_CONFIG = {\n",
        "    \"vocab_size\": 50257,     # Vocabulary size\n",
        "    \"context_length\": 1024,  # Context length\n",
        "    \"drop_rate\": 0.0,        # Dropout rate\n",
        "    \"qkv_bias\": True         # Query-key-value bias\n",
        "}\n",
        "\n",
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}\n",
        "\n",
        "CHOOSE_MODEL = \"gpt2-large (774M)\"\n",
        "\n",
        "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
        "\n",
        "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
        "settings, params = download_and_load_gpt2(\n",
        "    model_size=model_size,\n",
        "    models_dir=\"gpt2\"\n",
        ")\n",
        "\n",
        "model = GPTModel(BASE_CONFIG)\n",
        "load_weights_into_gpt(model, params)\n",
        "model.eval();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "IgaMoRztuEHi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65dd2765-6a4d-424c-9d8c-09d363b5a775"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "input_text = format_input(val_data[0])\n",
        "print(input_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "GDW-NtmtuICF"
      },
      "outputs": [],
      "source": [
        "token_ids = generate(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(input_text, tokenizer),\n",
        "    max_new_tokens=35,\n",
        "    context_size=BASE_CONFIG[\"context_length\"],\n",
        "    eos_id=50256,\n",
        ")\n",
        "generated_text = token_ids_to_text(token_ids, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "plwPU10euKuR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13a413e6-ae54-4c76-95e3-904ad0c67dd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Response:\n",
            "\n",
            "The chef cooks the meal every day.\n",
            "\n",
            "### Instructions:\n",
            "\n",
            "Write a response that appropriately completes the request.\n",
            "\n",
            "### Response\n"
          ]
        }
      ],
      "source": [
        "response_text = generated_text[len(input_text):].strip()\n",
        "print(response_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "qZKo0UvvuTF7"
      },
      "outputs": [],
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "    return loss\n",
        "\n",
        "\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        # Reduce the number of batches to match the total number of batches in the data loader\n",
        "        # if num_batches exceeds the number of batches in the data loader\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches\n",
        "\n",
        "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                       eval_freq, eval_iter, start_context, tokenizer):\n",
        "    # Initialize lists to track losses and tokens seen\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward() # Calculate loss gradients\n",
        "            optimizer.step() # Update model weights using loss gradients\n",
        "            tokens_seen += input_batch.numel() # Returns the total number of elements (or tokens) in the input_batch.\n",
        "            global_step += 1\n",
        "\n",
        "            # Optional evaluation step\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # Print a sample text after each epoch\n",
        "        generate_and_print_sample(\n",
        "            model, tokenizer, device, start_context\n",
        "        )\n",
        "\n",
        "    return train_losses, val_losses, track_tokens_seen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "pfwGwx86uY6A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2aee960-1769-4bbb-bc16-8e25a6cc4010"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 3.7066574573516844\n",
            "Validation loss: 3.6140518188476562\n"
          ]
        }
      ],
      "source": [
        "model.to(device)\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "with torch.no_grad():\n",
        "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
        "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
        "\n",
        "print(\"Training loss:\", train_loss)\n",
        "print(\"Validation loss:\", val_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSscWqXLuY1I"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.1)\n",
        "\n",
        "num_epochs = 1\n",
        "\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
        "    start_context=format_input(val_data[0]), tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxumGoyWuYyy"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "\n",
        "\n",
        "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
        "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "\n",
        "    # Plot training and validation loss against epochs\n",
        "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
        "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    ax1.legend(loc=\"upper right\")\n",
        "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
        "\n",
        "    # Create a second x-axis for tokens seen\n",
        "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
        "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
        "    ax2.set_xlabel(\"Tokens seen\")\n",
        "\n",
        "    fig.tight_layout()  # Adjust layout to make room\n",
        "    plt.savefig(\"loss-plot.pdf\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wf0UXAqOuYn7"
      },
      "outputs": [],
      "source": [
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sX5CiaJGuYfQ"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "\n",
        "for entry in test_data[:3]:\n",
        "\n",
        "    input_text = format_input(entry)\n",
        "\n",
        "    token_ids = generate(\n",
        "        model=model,\n",
        "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
        "        max_new_tokens=256,\n",
        "        context_size=BASE_CONFIG[\"context_length\"],\n",
        "        eos_id=50256\n",
        "    )\n",
        "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    response_text = (\n",
        "        generated_text[len(input_text):]\n",
        "        .replace(\"### Response:\", \"\")\n",
        "        .strip()\n",
        ")\n",
        "\n",
        "    print(input_text)\n",
        "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
        "    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n",
        "    print(\"-------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biRoJfKIuxCV"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
        "\n",
        "    input_text = format_input(entry)\n",
        "\n",
        "    token_ids = generate(\n",
        "        model=model,\n",
        "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
        "        max_new_tokens=256,\n",
        "        context_size=BASE_CONFIG[\"context_length\"],\n",
        "        eos_id=50256\n",
        "    )\n",
        "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    response_text = generated_text[len(input_text):].replace(\"### Response:\", \"\").strip()\n",
        "\n",
        "    test_data[i][\"model_response\"] = response_text\n",
        "\n",
        "\n",
        "with open(\"instruction-data-with-response.json\", \"w\") as file:\n",
        "    json.dump(test_data, file, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VMZIb7luw6C"
      },
      "outputs": [],
      "source": [
        "print(test_data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUVTsHivu4AJ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "\n",
        "file_name = f\"{re.sub(r'[ ()]', '', CHOOSE_MODEL) }-sft.pth\"\n",
        "torch.save(model.state_dict(), file_name)\n",
        "print(f\"Model saved as {file_name}\")\n",
        "\n",
        "# Load model via\n",
        "# model.load_state_dict(torch.load(\"gpt2-medium355M-sft.pth\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6QHTJzPu7kE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DsM7RHHJu7Xv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhfz-293u7Nv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSowm3-UuSUj"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "-ZRzHHRX8n1w",
        "lFwgAqdhO-Xv",
        "4BdbyT8NJUki",
        "D36v2OGum9oW",
        "-YC4uwDLjnwx",
        "Qp9aqkksu3YD",
        "t9-lx5HbfM1Z",
        "e8G3vx3Rn0bJ",
        "45NNta7yBSvG",
        "E27ijflmjMyJ",
        "cjYXqiTxoo-x",
        "sm4foXU9AgsX",
        "bGmnpZc4vXnL",
        "r0uS27ccmgG9",
        "2TjD9z-5wV5k"
      ],
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "1biP2UuOzaktDcHDrioHcQyPjjm2acY-g",
      "authorship_tag": "ABX9TyOrvsuoNe5bOi+j/+ubBMD9",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}